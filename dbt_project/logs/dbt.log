

============================== 2022-10-12 23:14:59.953765 | 96798e3d-df39-4343-ac9f-a8de4d548d97 ==============================
[0m23:14:59.953778 [info ] [MainThread]: Running with dbt=1.2.0
[0m23:14:59.954273 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/angel.hernandez/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m23:14:59.954459 [debug] [MainThread]: Tracking: tracking
[0m23:14:59.981547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113058f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113007eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113007e80>]}
[0m23:15:01.570317 [debug] [MainThread]: Executing "git --help"
[0m23:15:01.595233 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m23:15:01.595980 [debug] [MainThread]: STDERR: "b''"
[0m23:15:01.606243 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m23:15:01.607342 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:15:01.617782 [debug] [MainThread]: On debug: select 1 as id
[0m23:15:02.802829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1113db250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x128c4cc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x128c4ca00>]}
[0m23:15:03.148022 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2022-10-12 23:27:38.933230 | 30cf173c-4f7c-4e27-9df9-242f0834cbcd ==============================
[0m23:27:38.933356 [info ] [MainThread]: Running with dbt=1.2.0
[0m23:27:38.935007 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/angel.hernandez/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:27:38.935229 [debug] [MainThread]: Tracking: tracking
[0m23:27:38.973163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125a0eac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125a0e490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125a0ef10>]}
[0m23:27:39.007412 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m23:27:39.007886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '30cf173c-4f7c-4e27-9df9-242f0834cbcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125a47880>]}
[0m23:27:39.065432 [debug] [MainThread]: Parsing macros/etc.sql
[0m23:27:39.070195 [debug] [MainThread]: Parsing macros/catalog.sql
[0m23:27:39.078593 [debug] [MainThread]: Parsing macros/adapters.sql
[0m23:27:39.112993 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m23:27:39.116388 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m23:27:39.119915 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m23:27:39.128962 [debug] [MainThread]: Parsing macros/materializations/copy.sql
[0m23:27:39.132633 [debug] [MainThread]: Parsing macros/materializations/incremental.sql
[0m23:27:39.153088 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m23:27:39.155187 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m23:27:39.155716 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m23:27:39.156564 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m23:27:39.157051 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m23:27:39.157754 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m23:27:39.158538 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m23:27:39.159750 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m23:27:39.161343 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m23:27:39.162010 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m23:27:39.162662 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m23:27:39.163343 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m23:27:39.163915 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m23:27:39.165585 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m23:27:39.166269 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m23:27:39.169558 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m23:27:39.175709 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m23:27:39.178718 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m23:27:39.180768 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m23:27:39.202657 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m23:27:39.218552 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m23:27:39.233700 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m23:27:39.241827 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m23:27:39.244124 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m23:27:39.246401 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m23:27:39.251761 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m23:27:39.276831 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m23:27:39.278783 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m23:27:39.290958 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m23:27:39.312654 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m23:27:39.319351 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m23:27:39.322908 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m23:27:39.330538 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m23:27:39.332226 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m23:27:39.336312 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m23:27:39.339326 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m23:27:39.348050 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m23:27:39.371350 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m23:27:39.375335 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m23:27:39.378300 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m23:27:39.380240 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m23:27:39.381307 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m23:27:39.382301 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m23:27:39.383136 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m23:27:39.384771 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m23:27:39.391770 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m23:27:39.402094 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m23:27:39.403175 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m23:27:39.404880 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m23:27:39.406127 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m23:27:39.407388 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m23:27:39.409019 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m23:27:39.410057 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m23:27:39.411378 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m23:27:39.412841 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m23:27:39.415763 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m23:27:39.417146 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m23:27:39.418318 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m23:27:39.419421 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m23:27:39.420508 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m23:27:39.421875 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m23:27:39.423060 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m23:27:39.424118 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m23:27:39.430229 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m23:27:39.431231 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m23:27:39.433085 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m23:27:39.435158 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m23:27:39.436230 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m23:27:39.440414 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m23:27:39.444109 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m23:27:39.462760 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m23:27:39.466543 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m23:27:39.483701 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m23:27:39.490159 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m23:27:39.498970 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m23:27:39.514933 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m23:27:39.867760 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m23:27:39.884356 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m23:27:39.983787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '30cf173c-4f7c-4e27-9df9-242f0834cbcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125b6a190>]}
[0m23:27:39.993338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '30cf173c-4f7c-4e27-9df9-242f0834cbcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125a4f5b0>]}
[0m23:27:39.993741 [info ] [MainThread]: Found 2 models, 4 tests, 0 snapshots, 0 analyses, 285 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:27:39.994115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '30cf173c-4f7c-4e27-9df9-242f0834cbcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125ac7160>]}
[0m23:27:39.995972 [info ] [MainThread]: 
[0m23:27:39.996862 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m23:27:39.998056 [debug] [ThreadPool]: Acquiring new bigquery connection "list_fivetran-hands-on-lab"
[0m23:27:39.998302 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:27:40.396224 [debug] [ThreadPool]: Acquiring new bigquery connection "create_fivetran-hands-on-lab_dbt_workshop_20221013_dev"
[0m23:27:40.396995 [debug] [ThreadPool]: Acquiring new bigquery connection "create_fivetran-hands-on-lab_dbt_workshop_20221013_dev"
[0m23:27:40.397275 [debug] [ThreadPool]: BigQuery adapter: Creating schema "fivetran-hands-on-lab.dbt_workshop_20221013_dev".
[0m23:27:40.397516 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:27:41.197816 [debug] [ThreadPool]: Acquiring new bigquery connection "list_fivetran-hands-on-lab_dbt_workshop_20221013_dev"
[0m23:27:41.198203 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:27:41.527762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '30cf173c-4f7c-4e27-9df9-242f0834cbcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125b64790>]}
[0m23:27:41.528717 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m23:27:41.529140 [info ] [MainThread]: 
[0m23:27:41.543146 [debug] [Thread-1  ]: Began running node model.dbt_workshop_20221013.my_first_dbt_model
[0m23:27:41.543636 [info ] [Thread-1  ]: 1 of 2 START table model dbt_workshop_20221013_dev.my_first_dbt_model .......... [RUN]
[0m23:27:41.544483 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_workshop_20221013.my_first_dbt_model"
[0m23:27:41.544702 [debug] [Thread-1  ]: Began compiling node model.dbt_workshop_20221013.my_first_dbt_model
[0m23:27:41.544916 [debug] [Thread-1  ]: Compiling model.dbt_workshop_20221013.my_first_dbt_model
[0m23:27:41.548960 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_workshop_20221013.my_first_dbt_model"
[0m23:27:41.550519 [debug] [Thread-1  ]: finished collecting timing info
[0m23:27:41.551071 [debug] [Thread-1  ]: Began executing node model.dbt_workshop_20221013.my_first_dbt_model
[0m23:27:41.605092 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_workshop_20221013.my_first_dbt_model"
[0m23:27:41.606802 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:27:41.613674 [debug] [Thread-1  ]: On model.dbt_workshop_20221013.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.dbt_workshop_20221013.my_first_dbt_model"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev`.`my_first_dbt_model`
  
  
  OPTIONS()
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
  
[0m23:27:44.180338 [debug] [Thread-1  ]: finished collecting timing info
[0m23:27:44.181096 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '30cf173c-4f7c-4e27-9df9-242f0834cbcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125cdaee0>]}
[0m23:27:44.181520 [info ] [Thread-1  ]: 1 of 2 OK created table model dbt_workshop_20221013_dev.my_first_dbt_model ..... [[32mCREATE TABLE (2.0 rows, 0 processed)[0m in 2.64s]
[0m23:27:44.182060 [debug] [Thread-1  ]: Finished running node model.dbt_workshop_20221013.my_first_dbt_model
[0m23:27:44.182752 [debug] [Thread-3  ]: Began running node model.dbt_workshop_20221013.my_second_dbt_model
[0m23:27:44.183099 [info ] [Thread-3  ]: 2 of 2 START view model dbt_workshop_20221013_dev.my_second_dbt_model .......... [RUN]
[0m23:27:44.184000 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_workshop_20221013.my_second_dbt_model"
[0m23:27:44.184338 [debug] [Thread-3  ]: Began compiling node model.dbt_workshop_20221013.my_second_dbt_model
[0m23:27:44.184557 [debug] [Thread-3  ]: Compiling model.dbt_workshop_20221013.my_second_dbt_model
[0m23:27:44.188011 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_workshop_20221013.my_second_dbt_model"
[0m23:27:44.188581 [debug] [Thread-3  ]: finished collecting timing info
[0m23:27:44.188798 [debug] [Thread-3  ]: Began executing node model.dbt_workshop_20221013.my_second_dbt_model
[0m23:27:44.228353 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_workshop_20221013.my_second_dbt_model"
[0m23:27:44.228994 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m23:27:44.235565 [debug] [Thread-3  ]: On model.dbt_workshop_20221013.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.dbt_workshop_20221013.my_second_dbt_model"} */


  create or replace view `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev`.`my_second_dbt_model`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev`.`my_first_dbt_model`
where id = 1;


[0m23:27:45.444841 [debug] [Thread-3  ]: finished collecting timing info
[0m23:27:45.445692 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '30cf173c-4f7c-4e27-9df9-242f0834cbcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125d0ef10>]}
[0m23:27:45.446131 [info ] [Thread-3  ]: 2 of 2 OK created view model dbt_workshop_20221013_dev.my_second_dbt_model ..... [[32mOK[0m in 1.26s]
[0m23:27:45.446680 [debug] [Thread-3  ]: Finished running node model.dbt_workshop_20221013.my_second_dbt_model
[0m23:27:45.449588 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m23:27:45.450393 [info ] [MainThread]: 
[0m23:27:45.450829 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 5.45 seconds (5.45s).
[0m23:27:45.451192 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:27:45.451380 [debug] [MainThread]: Connection 'model.dbt_workshop_20221013.my_first_dbt_model' was properly closed.
[0m23:27:45.451557 [debug] [MainThread]: Connection 'model.dbt_workshop_20221013.my_second_dbt_model' was properly closed.
[0m23:27:45.465111 [info ] [MainThread]: 
[0m23:27:45.465862 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:27:45.466481 [info ] [MainThread]: 
[0m23:27:45.466964 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m23:27:45.467606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125bda3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125be2340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125d49070>]}


============================== 2022-10-13 16:17:47.144879 | 28875c1d-2082-4abd-a550-1105a16d5d44 ==============================
[0m16:17:47.144890 [info ] [MainThread]: Running with dbt=1.2.0
[0m16:17:47.145641 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/angel.hernandez/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m16:17:47.145828 [debug] [MainThread]: Tracking: tracking
[0m16:17:47.168752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f17eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ea4310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ec8d90>]}
[0m16:17:48.099111 [debug] [MainThread]: Executing "git --help"
[0m16:17:48.116740 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:17:48.118076 [debug] [MainThread]: STDERR: "b''"
[0m16:17:48.127470 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m16:17:48.128807 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:17:48.138373 [debug] [MainThread]: On debug: select 1 as id
[0m16:17:49.143503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bb44e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bb448e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bb44310>]}
[0m16:17:49.744891 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2022-10-13 16:26:11.627020 | d09d9084-eb1c-4b06-a6b1-350a368432b0 ==============================
[0m16:26:11.627033 [info ] [MainThread]: Running with dbt=1.2.0
[0m16:26:11.627747 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/angel.hernandez/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'deps', 'rpc_method': 'deps', 'indirect_selection': 'eager'}
[0m16:26:11.627915 [debug] [MainThread]: Tracking: tracking
[0m16:26:11.650995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070b1f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070b1970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070b19d0>]}
[0m16:26:11.654178 [debug] [MainThread]: Set downloads directory='/var/folders/mk/2ztnrxy17z11rwb071p5c8c80000gp/T/dbt-downloads-ijya38ni'
[0m16:26:11.655242 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m16:26:11.772326 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m16:26:11.772754 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_log.json
[0m16:26:11.853801 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_log.json 200
[0m16:26:11.859584 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json
[0m16:26:11.943845 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json 200
[0m16:26:11.962330 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/spark_utils.json
[0m16:26:12.049989 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/spark_utils.json 200
[0m16:26:12.054312 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m16:26:12.146622 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m16:26:12.243370 [info ] [MainThread]: Installing fivetran/fivetran_log
[0m16:26:12.614258 [info ] [MainThread]:   Installed from version 0.6.3
[0m16:26:12.614670 [info ] [MainThread]:   Up to date!
[0m16:26:12.615071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'd09d9084-eb1c-4b06-a6b1-350a368432b0', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070b6f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070b6fa0>]}
[0m16:26:12.615345 [info ] [MainThread]: Installing fivetran/fivetran_utils
[0m16:26:12.845492 [info ] [MainThread]:   Installed from version 0.3.9
[0m16:26:12.846312 [info ] [MainThread]:   Updated version available: 0.4.0
[0m16:26:12.847057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'd09d9084-eb1c-4b06-a6b1-350a368432b0', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070d9670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070b6e80>]}
[0m16:26:12.847518 [info ] [MainThread]: Installing dbt-labs/spark_utils
[0m16:26:13.184207 [info ] [MainThread]:   Installed from version 0.3.0
[0m16:26:13.184649 [info ] [MainThread]:   Up to date!
[0m16:26:13.185052 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'd09d9084-eb1c-4b06-a6b1-350a368432b0', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10717d520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10717d9d0>]}
[0m16:26:13.185365 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m16:26:13.557698 [info ] [MainThread]:   Installed from version 0.8.6
[0m16:26:13.558093 [info ] [MainThread]:   Updated version available: 0.9.2
[0m16:26:13.558452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'd09d9084-eb1c-4b06-a6b1-350a368432b0', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10717df70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10717d910>]}
[0m16:26:13.558759 [info ] [MainThread]: 
[0m16:26:13.559069 [info ] [MainThread]: Updates available for packages: ['fivetran/fivetran_utils', 'dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
[0m16:26:13.561652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107045130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ff9340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10706c0a0>]}


============================== 2022-10-13 16:31:25.190427 | b103913d-822b-4169-acb7-6fcb37cac0b0 ==============================
[0m16:31:25.190478 [info ] [MainThread]: Running with dbt=1.2.0
[0m16:31:25.192115 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/angel.hernandez/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['fivetran_log'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m16:31:25.192672 [debug] [MainThread]: Tracking: tracking
[0m16:31:25.233496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b5b5490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b5b5eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b5b5e50>]}
[0m16:31:25.456760 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:31:25.457103 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:31:25.470690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b103913d-822b-4169-acb7-6fcb37cac0b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b9610d0>]}
[0m16:31:25.498808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b103913d-822b-4169-acb7-6fcb37cac0b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b7ba130>]}
[0m16:31:25.499257 [info ] [MainThread]: Found 16 models, 20 tests, 0 snapshots, 0 analyses, 622 macros, 0 operations, 0 seed files, 7 sources, 0 exposures, 0 metrics
[0m16:31:25.499703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b103913d-822b-4169-acb7-6fcb37cac0b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b7c7df0>]}
[0m16:31:25.502876 [info ] [MainThread]: 
[0m16:31:25.503853 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m16:31:25.506237 [debug] [ThreadPool]: Acquiring new bigquery connection "list_fivetran-hands-on-lab"
[0m16:31:25.507032 [debug] [ThreadPool]: Acquiring new bigquery connection "list_fivetran-hands-on-lab"
[0m16:31:25.507429 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:31:25.507863 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:31:26.087302 [debug] [ThreadPool]: Acquiring new bigquery connection "create_fivetran-hands-on-lab_dbt_workshop_20221013_dev_stg_fivetran_log"
[0m16:31:26.087988 [debug] [ThreadPool]: Acquiring new bigquery connection "create_fivetran-hands-on-lab_dbt_workshop_20221013_dev_fivetran_log"
[0m16:31:26.088777 [debug] [ThreadPool]: Acquiring new bigquery connection "create_fivetran-hands-on-lab_dbt_workshop_20221013_dev_stg_fivetran_log"
[0m16:31:26.089466 [debug] [ThreadPool]: Acquiring new bigquery connection "create_fivetran-hands-on-lab_dbt_workshop_20221013_dev_fivetran_log"
[0m16:31:26.089763 [debug] [ThreadPool]: BigQuery adapter: Creating schema "fivetran-hands-on-lab.dbt_workshop_20221013_dev_stg_fivetran_log".
[0m16:31:26.090084 [debug] [ThreadPool]: BigQuery adapter: Creating schema "fivetran-hands-on-lab.dbt_workshop_20221013_dev_fivetran_log".
[0m16:31:26.090404 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:31:26.090702 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:31:26.793606 [debug] [ThreadPool]: Acquiring new bigquery connection "list_fivetran-hands-on-lab_dbt_workshop_20221013_dev_stg_fivetran_log"
[0m16:31:26.794485 [debug] [ThreadPool]: Acquiring new bigquery connection "list_fivetran-hands-on-lab_dbt_workshop_20221013_dev_fivetran_log"
[0m16:31:26.795253 [debug] [ThreadPool]: Acquiring new bigquery connection "list_fivetran-hands-on-lab_dbt_workshop_20221013_dev"
[0m16:31:26.795557 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:31:26.795830 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:31:26.796071 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:31:27.255793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b103913d-822b-4169-acb7-6fcb37cac0b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b9612b0>]}
[0m16:31:27.256630 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m16:31:27.257026 [info ] [MainThread]: 
[0m16:31:27.269454 [debug] [Thread-1  ]: Began running node model.fivetran_log.stg_fivetran_log__account
[0m16:31:27.269917 [debug] [Thread-2  ]: Began running node model.fivetran_log.stg_fivetran_log__active_volume
[0m16:31:27.270389 [info ] [Thread-1  ]: 1 of 14 START table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__account  [RUN]
[0m16:31:27.270827 [debug] [Thread-3  ]: Began running node model.fivetran_log.stg_fivetran_log__connector_tmp
[0m16:31:27.271304 [debug] [Thread-4  ]: Began running node model.fivetran_log.stg_fivetran_log__credits_used
[0m16:31:27.271767 [info ] [Thread-2  ]: 2 of 14 START table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__active_volume  [RUN]
[0m16:31:27.272004 [debug] [Thread-5  ]: Began running node model.fivetran_log.stg_fivetran_log__destination
[0m16:31:27.272279 [debug] [Thread-6  ]: Began running node model.fivetran_log.stg_fivetran_log__log
[0m16:31:27.272600 [debug] [Thread-7  ]: Began running node model.fivetran_log.stg_fivetran_log__usage_cost
[0m16:31:27.273873 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__account"
[0m16:31:27.274458 [info ] [Thread-3  ]: 3 of 14 START view model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__connector_tmp  [RUN]
[0m16:31:27.275049 [info ] [Thread-4  ]: 4 of 14 START table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__credits_used  [RUN]
[0m16:31:27.275902 [info ] [Thread-5  ]: 5 of 14 START table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__destination  [RUN]
[0m16:31:27.276735 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__active_volume"
[0m16:31:27.277543 [info ] [Thread-6  ]: 6 of 14 START table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__log  [RUN]
[0m16:31:27.277990 [info ] [Thread-7  ]: 7 of 14 START table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__usage_cost  [RUN]
[0m16:31:27.278292 [debug] [Thread-1  ]: Began compiling node model.fivetran_log.stg_fivetran_log__account
[0m16:31:27.279399 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__connector_tmp"
[0m16:31:27.280563 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__credits_used"
[0m16:31:27.281416 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__destination"
[0m16:31:27.281699 [debug] [Thread-2  ]: Began compiling node model.fivetran_log.stg_fivetran_log__active_volume
[0m16:31:27.284996 [debug] [Thread-2  ]: Compiling model.fivetran_log.stg_fivetran_log__active_volume
[0m16:31:27.282546 [debug] [Thread-6  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__log"
[0m16:31:27.283888 [debug] [Thread-1  ]: Compiling model.fivetran_log.stg_fivetran_log__account
[0m16:31:27.284189 [debug] [Thread-3  ]: Began compiling node model.fivetran_log.stg_fivetran_log__connector_tmp
[0m16:31:27.284464 [debug] [Thread-4  ]: Began compiling node model.fivetran_log.stg_fivetran_log__credits_used
[0m16:31:27.284746 [debug] [Thread-5  ]: Began compiling node model.fivetran_log.stg_fivetran_log__destination
[0m16:31:27.290733 [debug] [Thread-6  ]: Began compiling node model.fivetran_log.stg_fivetran_log__log
[0m16:31:27.296526 [debug] [Thread-2  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__active_volume"
[0m16:31:27.283608 [debug] [Thread-7  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__usage_cost"
[0m16:31:27.298910 [debug] [Thread-3  ]: Compiling model.fivetran_log.stg_fivetran_log__connector_tmp
[0m16:31:27.299629 [debug] [Thread-4  ]: Compiling model.fivetran_log.stg_fivetran_log__credits_used
[0m16:31:27.299926 [debug] [Thread-5  ]: Compiling model.fivetran_log.stg_fivetran_log__destination
[0m16:31:27.300521 [debug] [Thread-6  ]: Compiling model.fivetran_log.stg_fivetran_log__log
[0m16:31:27.300959 [debug] [Thread-7  ]: Began compiling node model.fivetran_log.stg_fivetran_log__usage_cost
[0m16:31:27.309479 [debug] [Thread-3  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__connector_tmp"
[0m16:31:27.326380 [debug] [Thread-4  ]: On "model.fivetran_log.stg_fivetran_log__credits_used": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:31:27.334332 [debug] [Thread-5  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__destination"
[0m16:31:27.341886 [debug] [Thread-6  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__log"
[0m16:31:27.342384 [debug] [Thread-2  ]: finished collecting timing info
[0m16:31:27.342625 [debug] [Thread-7  ]: Compiling model.fivetran_log.stg_fivetran_log__usage_cost
[0m16:31:27.343790 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m16:31:27.344393 [debug] [Thread-3  ]: finished collecting timing info
[0m16:31:27.344686 [debug] [Thread-2  ]: Began executing node model.fivetran_log.stg_fivetran_log__active_volume
[0m16:31:27.351828 [debug] [Thread-7  ]: On "model.fivetran_log.stg_fivetran_log__usage_cost": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:31:27.352186 [debug] [Thread-5  ]: finished collecting timing info
[0m16:31:27.352563 [debug] [Thread-6  ]: finished collecting timing info
[0m16:31:27.353268 [debug] [Thread-3  ]: Began executing node model.fivetran_log.stg_fivetran_log__connector_tmp
[0m16:31:27.359981 [debug] [Thread-7  ]: Opening a new connection, currently in state init
[0m16:31:27.367308 [debug] [Thread-5  ]: Began executing node model.fivetran_log.stg_fivetran_log__destination
[0m16:31:27.387289 [debug] [Thread-6  ]: Began executing node model.fivetran_log.stg_fivetran_log__log
[0m16:31:27.448082 [debug] [Thread-1  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__account"
[0m16:31:27.467421 [debug] [Thread-2  ]: Writing runtime SQL for node "model.fivetran_log.stg_fivetran_log__active_volume"
[0m16:31:27.479906 [debug] [Thread-5  ]: Writing runtime SQL for node "model.fivetran_log.stg_fivetran_log__destination"
[0m16:31:27.481315 [debug] [Thread-3  ]: Writing runtime SQL for node "model.fivetran_log.stg_fivetran_log__connector_tmp"
[0m16:31:27.486823 [debug] [Thread-6  ]: Writing runtime SQL for node "model.fivetran_log.stg_fivetran_log__log"
[0m16:31:27.489586 [debug] [Thread-1  ]: finished collecting timing info
[0m16:31:27.489874 [debug] [Thread-1  ]: Began executing node model.fivetran_log.stg_fivetran_log__account
[0m16:31:27.497016 [debug] [Thread-1  ]: Writing runtime SQL for node "model.fivetran_log.stg_fivetran_log__account"
[0m16:31:27.498022 [debug] [Thread-6  ]: Opening a new connection, currently in state init
[0m16:31:27.498329 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m16:31:27.498672 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m16:31:27.498969 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m16:31:27.500346 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:31:27.507280 [debug] [Thread-5  ]: On model.fivetran_log.stg_fivetran_log__destination: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__destination"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__destination`
  
  
  OPTIONS()
  as (
    with destination as (

    select * 
    from `fivetran-hands-on-lab`.`fivetran_log`.`destination`
),

fields as (

    select
        id as destination_id,
        account_id,
        cast(created_at as 
    timestamp
) as created_at,
        name as destination_name,
        region
    from destination
)

select * 
from fields
  );
  
[0m16:31:27.508425 [debug] [Thread-2  ]: On model.fivetran_log.stg_fivetran_log__active_volume: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__active_volume"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__active_volume`
  
  
  OPTIONS()
  as (
    with active_volume as (

    select * from `fivetran-hands-on-lab`.`fivetran_log`.`active_volume`
),

fields as (

    select
        id as active_volume_id,
        connector_id as connector_name, -- Note: this misnomer will be changed by Fivetran soon
        destination_id,
        cast(measured_at as 
    timestamp
) as measured_at,
        monthly_active_rows,
        schema_name,
        table_name
    from active_volume
)

select * 
from fields
  );
  
[0m16:31:27.510417 [debug] [Thread-6  ]: On model.fivetran_log.stg_fivetran_log__log: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__log"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__log`
  
  
  OPTIONS()
  as (
    with log as (

    select * 
    from `fivetran-hands-on-lab`.`fivetran_log`.`log`
),

fields as (

    select
        id as log_id, 
        cast(time_stamp as 
    timestamp
) as created_at,
        connector_id, -- Note: the connector_id column used to erroneously equal the connector_name, NOT its id.
        case when transformation_id is not null and event is null then 'TRANSFORMATION'
        else event end as event_type, 
        message_data,
        case 
        when transformation_id is not null and message_data like '%has succeeded%' then 'transformation run success'
        when transformation_id is not null and message_data like '%has failed%' then 'transformation run failed'
        else message_event end as event_subtype,
        transformation_id
    from log
)

select * 
from fields
  );
  
[0m16:31:27.511707 [debug] [Thread-1  ]: On model.fivetran_log.stg_fivetran_log__account: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__account"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__account`
  
  
  OPTIONS()
  as (
    with account as (
    
    select * 
    from `fivetran-hands-on-lab`.`fivetran_log`.`account`
),

fields as (

    select
        id as account_id,
        country,
        cast(created_at as 
    timestamp
) as created_at,
        name as account_name,
        status
    from account
)

select * 
from fields
  );
  
[0m16:31:27.515318 [debug] [Thread-3  ]: On model.fivetran_log.stg_fivetran_log__connector_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__connector_tmp"} */


  create or replace view `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__connector_tmp`
  OPTIONS()
  as select *
from `fivetran-hands-on-lab`.`fivetran_log`.`connector`;


[0m16:31:27.811314 [debug] [Thread-4  ]: On "model.fivetran_log.stg_fivetran_log__credits_used": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:31:27.926210 [debug] [Thread-2  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__active_volume"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__active_volume`
  
  
  OPTIONS()
  as (
    with active_volume as (

    select * from `fivetran-hands-on-lab`.`fivetran_log`.`active_volume`
),

fields as (

    select
        id as active_volume_id,
        connector_id as connector_name, -- Note: this misnomer will be changed by Fivetran soon
        destination_id,
        cast(measured_at as 
    timestamp
) as measured_at,
        monthly_active_rows,
        schema_name,
        table_name
    from active_volume
)

select * 
from fields
  );
  
[0m16:31:27.926489 [debug] [Thread-2  ]: BigQuery adapter: 404 Not found: Table fivetran-hands-on-lab:fivetran_log.active_volume was not found in location US

Location: US
Job ID: b4b4cb0e-966e-44b2-a6fc-4de573b0bea4

[0m16:31:27.926877 [debug] [Thread-2  ]: finished collecting timing info
[0m16:31:27.927558 [debug] [Thread-2  ]: Runtime Error in model stg_fivetran_log__active_volume (models/staging/stg_fivetran_log__active_volume.sql)
  404 Not found: Table fivetran-hands-on-lab:fivetran_log.active_volume was not found in location US
  
  Location: US
  Job ID: b4b4cb0e-966e-44b2-a6fc-4de573b0bea4
  
[0m16:31:27.927939 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b103913d-822b-4169-acb7-6fcb37cac0b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bb9cbe0>]}
[0m16:31:27.928351 [error] [Thread-2  ]: 2 of 14 ERROR creating table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__active_volume  [[31mERROR[0m in 0.65s]
[0m16:31:27.929053 [debug] [Thread-2  ]: Finished running node model.fivetran_log.stg_fivetran_log__active_volume
[0m16:31:27.970071 [debug] [Thread-7  ]: On "model.fivetran_log.stg_fivetran_log__usage_cost": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:31:28.057720 [debug] [Thread-4  ]: On "model.fivetran_log.stg_fivetran_log__credits_used": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:31:28.237105 [debug] [Thread-4  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__credits_used"
[0m16:31:28.237734 [debug] [Thread-4  ]: finished collecting timing info
[0m16:31:28.237985 [debug] [Thread-4  ]: Began executing node model.fivetran_log.stg_fivetran_log__credits_used
[0m16:31:28.243196 [debug] [Thread-4  ]: Writing runtime SQL for node "model.fivetran_log.stg_fivetran_log__credits_used"
[0m16:31:28.245659 [debug] [Thread-4  ]: On model.fivetran_log.stg_fivetran_log__credits_used: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__credits_used"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__credits_used`
  
  
  OPTIONS()
  as (
    

select
    cast(null as 
    string
) as destination_id,
    cast(null as 
    string
) as measured_month,
    cast(null as 
    int64
) as credits_spent


  );
  
[0m16:31:28.261266 [debug] [Thread-7  ]: On "model.fivetran_log.stg_fivetran_log__usage_cost": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:31:28.450378 [debug] [Thread-7  ]: On "model.fivetran_log.stg_fivetran_log__usage_cost": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:31:28.637398 [debug] [Thread-7  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__usage_cost"
[0m16:31:28.638020 [debug] [Thread-7  ]: finished collecting timing info
[0m16:31:28.638262 [debug] [Thread-7  ]: Began executing node model.fivetran_log.stg_fivetran_log__usage_cost
[0m16:31:28.643112 [debug] [Thread-7  ]: Writing runtime SQL for node "model.fivetran_log.stg_fivetran_log__usage_cost"
[0m16:31:28.643730 [debug] [Thread-7  ]: On model.fivetran_log.stg_fivetran_log__usage_cost: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__usage_cost"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__usage_cost`
  
  
  OPTIONS()
  as (
    

select
    cast(null as 
    string
) as destination_id,
    cast(null as 
    string
) as measured_month,
    cast(null as 
    int64
) as dollars_spent


  );
  
[0m16:31:28.703075 [debug] [Thread-3  ]: finished collecting timing info
[0m16:31:28.703914 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b103913d-822b-4169-acb7-6fcb37cac0b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b9ea7f0>]}
[0m16:31:28.704344 [info ] [Thread-3  ]: 3 of 14 OK created view model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__connector_tmp  [[32mOK[0m in 1.43s]
[0m16:31:28.704864 [debug] [Thread-3  ]: Finished running node model.fivetran_log.stg_fivetran_log__connector_tmp
[0m16:31:28.705340 [debug] [Thread-9  ]: Began running node model.fivetran_log.stg_fivetran_log__connector
[0m16:31:28.705690 [info ] [Thread-9  ]: 8 of 14 START table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__connector  [RUN]
[0m16:31:28.706546 [debug] [Thread-9  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__connector"
[0m16:31:28.706774 [debug] [Thread-9  ]: Began compiling node model.fivetran_log.stg_fivetran_log__connector
[0m16:31:28.706983 [debug] [Thread-9  ]: Compiling model.fivetran_log.stg_fivetran_log__connector
[0m16:31:28.713604 [debug] [Thread-9  ]: Opening a new connection, currently in state init
[0m16:31:29.191872 [debug] [Thread-9  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__connector"
[0m16:31:29.192573 [debug] [Thread-9  ]: finished collecting timing info
[0m16:31:29.192862 [debug] [Thread-9  ]: Began executing node model.fivetran_log.stg_fivetran_log__connector
[0m16:31:29.197835 [debug] [Thread-9  ]: Writing runtime SQL for node "model.fivetran_log.stg_fivetran_log__connector"
[0m16:31:29.198543 [debug] [Thread-9  ]: On model.fivetran_log.stg_fivetran_log__connector: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__connector"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__connector`
  
  
  OPTIONS()
  as (
    with connector as (

    select * 
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__connector_tmp`
),

fields as (
    select
        
    
    
    _fivetran_deleted
    
 as 
    
    _fivetran_deleted
    
, 
    
    
    _fivetran_synced
    
 as 
    
    _fivetran_synced
    
, 
    
    
    connecting_user_id
    
 as 
    
    connecting_user_id
    
, 
    
    
    connector_id
    
 as 
    
    connector_id
    
, 
    
    
    connector_name
    
 as 
    
    connector_name
    
, 
    cast(null as 
    string
) as 
    
    connector_type
    
 , 
    
    
    connector_type_id
    
 as 
    
    connector_type_id
    
, 
    
    
    destination_id
    
 as 
    
    destination_id
    
, 
    
    
    paused
    
 as 
    
    paused
    
, 
    cast(null as 
    int64
) as 
    
    service_version
    
 , 
    
    
    signed_up
    
 as 
    
    signed_up
    



        ,row_number() over ( partition by connector_name, destination_id order by _fivetran_synced desc ) as nth_last_record
    from connector
),

final as (

    select 
        connector_id,
        connector_name,
        coalesce(connector_type_id, connector_type) as connector_type,
        destination_id,
        connecting_user_id,
        paused as is_paused,
        signed_up as set_up_at
    from fields

    -- Only look at the most recent one
    where nth_last_record = 1
)

select * 
from final
  );
  
[0m16:31:30.646132 [debug] [Thread-5  ]: finished collecting timing info
[0m16:31:30.646888 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b103913d-822b-4169-acb7-6fcb37cac0b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b9eac10>]}
[0m16:31:30.647316 [info ] [Thread-5  ]: 5 of 14 OK created table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__destination  [[32mCREATE TABLE (1.0 rows, 68.0 Bytes processed)[0m in 3.37s]
[0m16:31:30.647872 [debug] [Thread-5  ]: Finished running node model.fivetran_log.stg_fivetran_log__destination
[0m16:31:30.670994 [debug] [Thread-1  ]: finished collecting timing info
[0m16:31:30.672071 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b103913d-822b-4169-acb7-6fcb37cac0b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b9ea5e0>]}
[0m16:31:30.673210 [info ] [Thread-1  ]: 1 of 14 OK created table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__account  [[32mCREATE TABLE (1.0 rows, 47.0 Bytes processed)[0m in 3.40s]
[0m16:31:30.674095 [debug] [Thread-1  ]: Finished running node model.fivetran_log.stg_fivetran_log__account
[0m16:31:30.897511 [debug] [Thread-4  ]: finished collecting timing info
[0m16:31:30.898250 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b103913d-822b-4169-acb7-6fcb37cac0b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b9ea9d0>]}
[0m16:31:30.898721 [info ] [Thread-4  ]: 4 of 14 OK created table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__credits_used  [[32mCREATE TABLE (1.0 rows, 0 processed)[0m in 3.62s]
[0m16:31:30.899328 [debug] [Thread-4  ]: Finished running node model.fivetran_log.stg_fivetran_log__credits_used
[0m16:31:30.924042 [debug] [Thread-6  ]: finished collecting timing info
[0m16:31:30.924910 [debug] [Thread-6  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b103913d-822b-4169-acb7-6fcb37cac0b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b9eadf0>]}
[0m16:31:30.925341 [info ] [Thread-6  ]: 6 of 14 OK created table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__log  [[32mCREATE TABLE (14.5k rows, 2.4 MB processed)[0m in 3.64s]
[0m16:31:30.925896 [debug] [Thread-6  ]: Finished running node model.fivetran_log.stg_fivetran_log__log
[0m16:31:31.675740 [debug] [Thread-7  ]: finished collecting timing info
[0m16:31:31.676558 [debug] [Thread-7  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b103913d-822b-4169-acb7-6fcb37cac0b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b9eafd0>]}
[0m16:31:31.676988 [info ] [Thread-7  ]: 7 of 14 OK created table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__usage_cost  [[32mCREATE TABLE (1.0 rows, 0 processed)[0m in 4.39s]
[0m16:31:31.677513 [debug] [Thread-7  ]: Finished running node model.fivetran_log.stg_fivetran_log__usage_cost
[0m16:31:32.666745 [debug] [Thread-9  ]: finished collecting timing info
[0m16:31:32.667505 [debug] [Thread-9  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b103913d-822b-4169-acb7-6fcb37cac0b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bc6cc70>]}
[0m16:31:32.667933 [info ] [Thread-9  ]: 8 of 14 OK created table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__connector  [[32mCREATE TABLE (1.0 rows, 98.0 Bytes processed)[0m in 3.96s]
[0m16:31:32.668602 [debug] [Thread-9  ]: Finished running node model.fivetran_log.stg_fivetran_log__connector
[0m16:31:32.669338 [debug] [Thread-11 ]: Began running node model.fivetran_log.fivetran_log__connector_status
[0m16:31:32.669834 [info ] [Thread-11 ]: 9 of 14 START table model dbt_workshop_20221013_dev_fivetran_log.fivetran_log__connector_status  [RUN]
[0m16:31:32.670108 [debug] [Thread-12 ]: Began running node model.fivetran_log.fivetran_log__mar_table_history
[0m16:31:32.671210 [debug] [Thread-11 ]: Acquiring new bigquery connection "model.fivetran_log.fivetran_log__connector_status"
[0m16:31:32.671472 [info ] [Thread-12 ]: 10 of 14 SKIP relation dbt_workshop_20221013_dev_fivetran_log.fivetran_log__mar_table_history  [[33mSKIP[0m]
[0m16:31:32.671765 [debug] [Thread-11 ]: Began compiling node model.fivetran_log.fivetran_log__connector_status
[0m16:31:32.672412 [debug] [Thread-12 ]: Finished running node model.fivetran_log.fivetran_log__mar_table_history
[0m16:31:32.672775 [debug] [Thread-11 ]: Compiling model.fivetran_log.fivetran_log__connector_status
[0m16:31:32.673397 [debug] [Thread-14 ]: Began running node model.fivetran_log.fivetran_log__usage_mar_destination_history
[0m16:31:32.691331 [info ] [Thread-14 ]: 11 of 14 SKIP relation dbt_workshop_20221013_dev_fivetran_log.fivetran_log__usage_mar_destination_history  [[33mSKIP[0m]
[0m16:31:32.691980 [debug] [Thread-14 ]: Finished running node model.fivetran_log.fivetran_log__usage_mar_destination_history
[0m16:31:32.723485 [debug] [Thread-11 ]: Writing injected SQL for node "model.fivetran_log.fivetran_log__connector_status"
[0m16:31:32.724134 [debug] [Thread-11 ]: finished collecting timing info
[0m16:31:32.724384 [debug] [Thread-11 ]: Began executing node model.fivetran_log.fivetran_log__connector_status
[0m16:31:32.729560 [debug] [Thread-11 ]: Writing runtime SQL for node "model.fivetran_log.fivetran_log__connector_status"
[0m16:31:32.730220 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m16:31:32.736913 [debug] [Thread-11 ]: On model.fivetran_log.fivetran_log__connector_status: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.fivetran_log__connector_status"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.`fivetran_log__connector_status`
  
  
  OPTIONS()
  as (
    with transformation_removal as (

    select *
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__log`
    where transformation_id is null

),

connector_log as (
    select 
        *,
        sum( case when event_subtype in ('sync_start') then 1 else 0 end) over ( partition by connector_id 
            order by created_at rows unbounded preceding) as sync_batch_id
    from transformation_removal
    -- only looking at errors, warnings, and syncs here
    where event_type = 'SEVERE'
        or event_type = 'WARNING'
        or event_subtype like 'sync%'
        or (event_subtype = 'status' 
            and 

 
  json_extract_scalar(message_data, '$.status')

 = 'RESCHEDULED'
            
            and 

 
  json_extract_scalar(message_data, '$.reason')

 like '%intended behavior%'
            ) -- for priority-first syncs. these should be captured by event_type = 'WARNING' but let's make sure
        or (event_subtype = 'status' 
            and 

 
  json_extract_scalar(message_data, '$.status')

 = 'SUCCESSFUL'
        )
        -- whole reason is "We have rescheduled the connector to force flush data from the forward sync into your destination. This is intended behavior and means that the connector is working as expected."
),

schema_changes as (

    select
        connector_id,
        count(*) as number_of_schema_changes_last_month

    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__log`

    where 
        datetime_diff(
        cast(
    current_timestamp
 as datetime),
        cast(created_at as datetime),
        day
    ) <= 30
        and event_subtype in ('create_table', 'alter_table', 'create_schema', 'change_schema_config')

    group by 1

),

connector as (

    select *
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__connector`

),

destination as (

    select * 
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__destination`
),

connector_metrics as (

    select
        connector.connector_id,
        connector.connector_name,
        connector.connector_type,
        connector.destination_id,
        connector.is_paused,
        connector.set_up_at,
        max(case when connector_log.event_subtype = 'sync_start' then connector_log.created_at else null end) as last_sync_started_at,

        max(case when connector_log.event_subtype = 'sync_end' 
            then connector_log.created_at else null end) as last_sync_completed_at,

        max(case when connector_log.event_subtype in ('status', 'sync_end')
                and 

 
  json_extract_scalar(connector_log.message_data, '$.status')

 ='SUCCESSFUL'
            then connector_log.created_at else null end) as last_successful_sync_completed_at,


        max(case when connector_log.event_subtype = 'sync_end' 
            then connector_log.sync_batch_id else null end) as last_sync_batch_id,

        max(case when connector_log.event_subtype in ('status', 'sync_end')
                and 

 
  json_extract_scalar(connector_log.message_data, '$.status')

 ='RESCHEDULED'
                and 

 
  json_extract_scalar(connector_log.message_data, '$.reason')

 like '%intended behavior%'
            then connector_log.created_at else null end) as last_priority_first_sync_completed_at,
                

        max(case when connector_log.event_type = 'SEVERE' then connector_log.created_at else null end) as last_error_at,

        max(case when connector_log.event_type = 'SEVERE' then connector_log.sync_batch_id else null end) as last_error_batch,
        max(case when event_type = 'WARNING' then connector_log.created_at else null end) as last_warning_at

    from connector 
    left join connector_log 
        on connector_log.connector_id = connector.connector_id
    group by 1,2,3,4,5,6

),

connector_health as (

    select
        *,
        case 
            -- connector is paused
            when is_paused then 'paused'

            -- a sync has never been attempted
            when last_sync_started_at is null then 'incomplete'

            -- a priority-first sync has occurred, but a normal sync has not
            when last_priority_first_sync_completed_at is not null and last_sync_completed_at is null then 'priority first sync'

            -- a priority sync has occurred more recently than a normal one (may occurr if the connector has been paused and resumed)
            when last_priority_first_sync_completed_at > last_sync_completed_at then 'priority first sync'

            -- a sync has been attempted, but not completed, and it's not due to errors. also a priority-first sync hasn't
            when last_sync_completed_at is null and last_error_at is null then 'initial sync in progress'

            -- the last attempted sync had an error
            when last_sync_batch_id = last_error_batch then 'broken'

            -- there's never been a successful sync and there have been errors
            when last_sync_completed_at is null and last_error_at is not null then 'broken'

        else 'connected' end as connector_health

    from connector_metrics
),

-- Joining with log to grab pertinent error/warning messagees
connector_recent_logs as (

    select 
        connector_health.connector_id,
        connector_health.connector_name,
        connector_health.connector_type,
        connector_health.destination_id,
        connector_health.connector_health,
        connector_health.last_successful_sync_completed_at,
        connector_health.last_sync_started_at,
        connector_health.last_sync_completed_at,
        connector_health.set_up_at,
        connector_log.event_subtype,
        connector_log.event_type,
        connector_log.message_data

    from connector_health 
    left join connector_log 
        on connector_log.connector_id = connector_health.connector_id
        -- limiting relevance to since the last successful sync completion (if there has been one)
        and connector_log.created_at > coalesce(connector_health.last_sync_completed_at, connector_health.last_priority_first_sync_completed_at, '2000-01-01') 
        -- only looking at errors and warnings (excluding syncs - both normal and priority first)
        and connector_log.event_type != 'INFO' 
        -- need to explicitly avoid priority first statuses because they are of event_type WARNING
        and not (connector_log.event_subtype = 'status' 
            and 

 
  json_extract_scalar(connector_log.message_data, '$.status')

 ='RESCHEDULED'
            and 

 
  json_extract_scalar(connector_log.message_data, '$.reason')

 like '%intended behavior%')

    group by 1,2,3,4,5,6,7,8,9,10,11,12 -- de-duping error messages
    

),

final as (

    select
        connector_recent_logs.connector_id,
        connector_recent_logs.connector_name,
        connector_recent_logs.connector_type,
        connector_recent_logs.destination_id,
        destination.destination_name,
        connector_recent_logs.connector_health,
        connector_recent_logs.last_successful_sync_completed_at,
        connector_recent_logs.last_sync_started_at,
        connector_recent_logs.last_sync_completed_at,
        connector_recent_logs.set_up_at,
        coalesce(schema_changes.number_of_schema_changes_last_month, 0) as number_of_schema_changes_last_month
        
        
        , 
    string_agg(distinct case when connector_recent_logs.event_type = 'SEVERE' then connector_recent_logs.message_data else null end, '\n')

 as errors_since_last_completed_sync
        , 
    string_agg(distinct case when connector_recent_logs.event_type = 'WARNING' then connector_recent_logs.message_data else null end, '\n')

 as warnings_since_last_completed_sync
        

    from connector_recent_logs
    left join schema_changes 
        on connector_recent_logs.connector_id = schema_changes.connector_id 

    join destination on destination.destination_id = connector_recent_logs.destination_id
    group by 1,2,3,4,5,6,7,8,9,10,11
)

select * from final
  );
  
[0m16:31:37.234259 [debug] [Thread-11 ]: finished collecting timing info
[0m16:31:37.235593 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b103913d-822b-4169-acb7-6fcb37cac0b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bcd2f10>]}
[0m16:31:37.236139 [info ] [Thread-11 ]: 9 of 14 OK created table model dbt_workshop_20221013_dev_fivetran_log.fivetran_log__connector_status  [[32mCREATE TABLE (1.0 rows, 2.0 MB processed)[0m in 4.56s]
[0m16:31:37.236818 [debug] [Thread-11 ]: Finished running node model.fivetran_log.fivetran_log__connector_status
[0m16:31:37.238030 [debug] [Thread-16 ]: Began running node model.fivetran_log.fivetran_log__audit_table
[0m16:31:37.238440 [debug] [Thread-17 ]: Began running node model.fivetran_log.fivetran_log__connector_daily_events
[0m16:31:37.239029 [info ] [Thread-16 ]: 12 of 14 START incremental model dbt_workshop_20221013_dev_fivetran_log.fivetran_log__audit_table  [RUN]
[0m16:31:37.239298 [debug] [Thread-18 ]: Began running node model.fivetran_log.fivetran_log__schema_changelog
[0m16:31:37.239706 [info ] [Thread-17 ]: 13 of 14 START table model dbt_workshop_20221013_dev_fivetran_log.fivetran_log__connector_daily_events  [RUN]
[0m16:31:37.240361 [info ] [Thread-18 ]: 14 of 14 START table model dbt_workshop_20221013_dev_fivetran_log.fivetran_log__schema_changelog  [RUN]
[0m16:31:37.241268 [debug] [Thread-16 ]: Acquiring new bigquery connection "model.fivetran_log.fivetran_log__audit_table"
[0m16:31:37.242381 [debug] [Thread-17 ]: Acquiring new bigquery connection "model.fivetran_log.fivetran_log__connector_daily_events"
[0m16:31:37.243530 [debug] [Thread-18 ]: Acquiring new bigquery connection "model.fivetran_log.fivetran_log__schema_changelog"
[0m16:31:37.243911 [debug] [Thread-16 ]: Began compiling node model.fivetran_log.fivetran_log__audit_table
[0m16:31:37.244173 [debug] [Thread-17 ]: Began compiling node model.fivetran_log.fivetran_log__connector_daily_events
[0m16:31:37.244452 [debug] [Thread-18 ]: Began compiling node model.fivetran_log.fivetran_log__schema_changelog
[0m16:31:37.244721 [debug] [Thread-16 ]: Compiling model.fivetran_log.fivetran_log__audit_table
[0m16:31:37.244958 [debug] [Thread-17 ]: Compiling model.fivetran_log.fivetran_log__connector_daily_events
[0m16:31:37.245253 [debug] [Thread-18 ]: Compiling model.fivetran_log.fivetran_log__schema_changelog
[0m16:31:37.417506 [debug] [Thread-18 ]: Writing injected SQL for node "model.fivetran_log.fivetran_log__schema_changelog"
[0m16:31:37.453690 [debug] [Thread-18 ]: finished collecting timing info
[0m16:31:37.456917 [debug] [Thread-17 ]: Opening a new connection, currently in state init
[0m16:31:37.457336 [debug] [Thread-18 ]: Began executing node model.fivetran_log.fivetran_log__schema_changelog
[0m16:31:37.484921 [debug] [Thread-16 ]: Writing injected SQL for node "model.fivetran_log.fivetran_log__audit_table"
[0m16:31:37.492622 [debug] [Thread-18 ]: Writing runtime SQL for node "model.fivetran_log.fivetran_log__schema_changelog"
[0m16:31:37.493367 [debug] [Thread-16 ]: finished collecting timing info
[0m16:31:37.493719 [debug] [Thread-16 ]: Began executing node model.fivetran_log.fivetran_log__audit_table
[0m16:31:37.511553 [debug] [Thread-17 ]: On model.fivetran_log.fivetran_log__connector_daily_events: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.fivetran_log__connector_daily_events"} */

    
        select  min( signed_up ) as min_date from `fivetran-hands-on-lab`.`fivetran_log`.`connector`
    
  
[0m16:31:37.518220 [debug] [Thread-18 ]: Opening a new connection, currently in state init
[0m16:31:37.561837 [debug] [Thread-16 ]: Writing runtime SQL for node "model.fivetran_log.fivetran_log__audit_table"
[0m16:31:37.562963 [debug] [Thread-16 ]: Opening a new connection, currently in state init
[0m16:31:37.565679 [debug] [Thread-18 ]: On model.fivetran_log.fivetran_log__schema_changelog: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.fivetran_log__schema_changelog"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.`fivetran_log__schema_changelog`
  
  
  OPTIONS()
  as (
    with schema_changes as (

    select *
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__log`

    where event_subtype in ('create_table', 'alter_table', 'create_schema', 'change_schema_config')
),

connector as (

    select *
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.`fivetran_log__connector_status`
),

add_connector_info as (

    select 
        schema_changes.*,
        connector.connector_name,
        connector.destination_id,
        connector.destination_name

    from schema_changes join connector using(connector_id)
),

final as (

    select
        connector_id,
        connector_name,
        destination_id,
        destination_name,
        created_at,
        event_subtype,
        message_data,

        case 
        when event_subtype = 'alter_table' then 

 
  json_extract_scalar(message_data, '$.table')

 
        when event_subtype = 'create_table' then 

 
  json_extract_scalar(message_data, '$.name')

 
        else null end as table_name,

        case 
        when event_subtype = 'create_schema' or event_subtype = 'create_table' then 

 
  json_extract_scalar(message_data, '$.schema')

 
        else null end as schema_name
    
    from add_connector_info
)

select * from final
order by created_at desc, connector_id
  );
  
[0m16:31:37.570764 [debug] [Thread-16 ]: On model.fivetran_log.fivetran_log__audit_table: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.fivetran_log__audit_table"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.`fivetran_log__audit_table`
  partition by timestamp_trunc(sync_start, day)
  
  OPTIONS()
  as (
    

with sync_log as (
    
    select 
        *,
        

 
  json_extract_scalar(message_data, '$.table')

 as table_name
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__log`
    where event_subtype in ('sync_start', 'sync_end', 'write_to_table_start', 'write_to_table_end', 'records_modified')

    
),


connector as (

    select *
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.`fivetran_log__connector_status`
),

add_connector_info as (

    select 
        sync_log.*,
        connector.connector_name,
        connector.destination_id,
        connector.destination_name
    from sync_log 
    left join connector
        on connector.connector_id = sync_log.connector_id
),

sync_timestamps as (

    select
        connector_id,
        connector_name,
        table_name,
        event_subtype,
        destination_id,
        destination_name,
        created_at as write_to_table_start,
        min(case when event_subtype = 'write_to_table_end' then created_at else null end) 
            over (partition by connector_id, table_name order by created_at ROWS between CURRENT ROW AND UNBOUNDED FOLLOWING) as write_to_table_end,

        max(case when event_subtype = 'sync_start' then created_at else null end) 
            over (partition by connector_id order by created_at ROWS between UNBOUNDED PRECEDING and CURRENT ROW) as sync_start,

        min(case when event_subtype = 'sync_end' then created_at else null end) 
            over (partition by connector_id order by created_at ROWS between CURRENT ROW AND UNBOUNDED FOLLOWING) as sync_end, -- coalesce with next_sync_start

        min(case when event_subtype = 'sync_start' then created_at else null end) 
            over (partition by connector_id order by created_at ROWS between CURRENT ROW AND UNBOUNDED FOLLOWING) as next_sync_start
    from add_connector_info
),

-- this will be the base for every record in the final CTE
limit_to_table_starts as (

    select *
    from sync_timestamps 
    where event_subtype = 'write_to_table_start'
),

records_modified_log as (

    select 
        connector_id,
        created_at,
        

 
  json_extract_scalar(message_data, '$.table')

 as table_name,
        

 
  json_extract_scalar(message_data, '$.schema')

 as schema_name,
        

 
  json_extract_scalar(message_data, '$.operationType')

 as operation_type,
        cast (

 
  json_extract_scalar(message_data, '$.count')

 as 
    int64
) as row_count
    from sync_log 
    where event_subtype = 'records_modified'

),

sum_records_modified as (

    select
        limit_to_table_starts.connector_id,
        limit_to_table_starts.connector_name,
        limit_to_table_starts.table_name,
        limit_to_table_starts.destination_id,
        limit_to_table_starts.destination_name,
        limit_to_table_starts.write_to_table_start,
        limit_to_table_starts.write_to_table_end,
        limit_to_table_starts.sync_start,
        case when limit_to_table_starts.sync_end > limit_to_table_starts.next_sync_start then null else limit_to_table_starts.sync_end end as sync_end,
        sum(case when records_modified_log.operation_type = 'REPLACED_OR_INSERTED' then records_modified_log.row_count else 0 end) as sum_rows_replaced_or_inserted,
        sum(case when records_modified_log.operation_type = 'UPDATED' then records_modified_log.row_count else 0 end) as sum_rows_updated,
        sum(case when records_modified_log.operation_type = 'DELETED' then records_modified_log.row_count else 0 end) as sum_rows_deleted
    from limit_to_table_starts
    left join records_modified_log on 
        limit_to_table_starts.connector_id = records_modified_log.connector_id
        and limit_to_table_starts.table_name = records_modified_log.table_name

        -- confine it to one sync
        and records_modified_log.created_at > limit_to_table_starts.sync_start 
        and records_modified_log.created_at < coalesce(limit_to_table_starts.sync_end, limit_to_table_starts.next_sync_start) 

    group by 1,2,3,4,5,6,7,8,9
),

surrogate_key as (

    select 
        *,
        to_hex(md5(cast(coalesce(cast(connector_id as 
    string
), '') || '-' || coalesce(cast(destination_id as 
    string
), '') || '-' || coalesce(cast(table_name as 
    string
), '') || '-' || coalesce(cast(write_to_table_start as 
    string
), '') as 
    string
))) as unique_table_sync_key
    from sum_records_modified
)

select *
from surrogate_key
  );
  
[0m16:31:38.906652 [debug] [Thread-17 ]: On model.fivetran_log.fivetran_log__connector_daily_events: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.fivetran_log__connector_daily_events"} */


        select datetime_diff(
        cast(

        datetime_add(
            cast( timestamp_trunc(
        cast(
    current_timestamp
 as timestamp),
        day
    ) as datetime),
        interval 1 week
        )

 as datetime),
        cast(cast('2022-07-13' as date) as datetime),
        day
    )
[0m16:31:40.198911 [debug] [Thread-17 ]: Writing injected SQL for node "model.fivetran_log.fivetran_log__connector_daily_events"
[0m16:31:40.199632 [debug] [Thread-17 ]: finished collecting timing info
[0m16:31:40.199889 [debug] [Thread-17 ]: Began executing node model.fivetran_log.fivetran_log__connector_daily_events
[0m16:31:40.205302 [debug] [Thread-17 ]: Writing runtime SQL for node "model.fivetran_log.fivetran_log__connector_daily_events"
[0m16:31:40.206027 [debug] [Thread-17 ]: On model.fivetran_log.fivetran_log__connector_daily_events: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.fivetran_log__connector_daily_events"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.`fivetran_log__connector_daily_events`
  
  
  OPTIONS()
  as (
    -- depends_on: `fivetran-hands-on-lab`.`fivetran_log`.`connector`

with connector as (
    
    select * 
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.`fivetran_log__connector_status`
),

-- grab api calls, schema changes, and record modifications
log_events as (

    select 
        connector_id,
        cast( timestamp_trunc(
        cast(created_at as timestamp),
        day
    ) as date) as date_day,
        case 
            when event_subtype in ('create_table', 'alter_table', 'create_schema', 'change_schema_config') then 'schema_change' 
            else event_subtype end as event_subtype,

        sum(case when event_subtype = 'records_modified' then cast( 

 
  json_extract_scalar(message_data, '$.count')

 as 
    int64
 )
        else 1 end) as count_events 

    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__log`

    where event_subtype in ('api_call', 
                            'records_modified', 
                            'create_table', 'alter_table', 'create_schema', 'change_schema_config') -- all schema changes
                            
        and connector_id is not null

    group by 1,2,3
),

pivot_out_events as (

    select
        connector_id,
        date_day,
        max(case when event_subtype = 'api_call' then count_events else 0 end) as count_api_calls,
        max(case when event_subtype = 'records_modified' then count_events else 0 end) as count_record_modifications,
        max(case when event_subtype = 'schema_change' then count_events else 0 end) as count_schema_changes

    from log_events
    group by 1,2
),

connector_event_counts as (

    select
        pivot_out_events.date_day,
        pivot_out_events.count_api_calls,
        pivot_out_events.count_record_modifications,
        pivot_out_events.count_schema_changes,
        connector.connector_name,
        connector.connector_id,
        connector.connector_type,
        connector.destination_name,
        connector.destination_id,
        connector.set_up_at
    from
    connector left join pivot_out_events 
        on pivot_out_events.connector_id = connector.connector_id
),

spine as (

    
    
    
    
    

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
    
    

    )

    select *
    from unioned
    where generated_number <= 99
    order by generated_number



),

all_periods as (

    select (
        

        datetime_add(
            cast( cast('2022-07-13' as date) as datetime),
        interval row_number() over (order by 1) - 1 day
        )


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= 

        datetime_add(
            cast( timestamp_trunc(
        cast(
    current_timestamp
 as timestamp),
        day
    ) as datetime),
        interval 1 week
        )



)

select * from filtered

 
),

connector_event_history as (

    select
        cast(spine.date_day as date) as date_day,
        connector_event_counts.connector_name,
        connector_event_counts.connector_id,
        connector_event_counts.connector_type,
        connector_event_counts.destination_name,
        connector_event_counts.destination_id,
        max(case 
            when cast(spine.date_day as date) = connector_event_counts.date_day then connector_event_counts.count_api_calls
            else 0
        end) as count_api_calls,
        max(case 
            when cast(spine.date_day as date) = connector_event_counts.date_day then connector_event_counts.count_record_modifications
            else 0
        end) as count_record_modifications,
        max(case 
            when cast(spine.date_day as date) = connector_event_counts.date_day then connector_event_counts.count_schema_changes
            else 0
        end) as count_schema_changes
    from
    spine join connector_event_counts
        on spine.date_day  >= cast( timestamp_trunc(
        cast(connector_event_counts.set_up_at as timestamp),
        day
    ) as date)

    group by 1,2,3,4,5,6
),

-- now rejoin spine to get a complete calendar
join_event_history as (
    
    select
        spine.date_day,
        connector_event_history.connector_name,
        connector_event_history.connector_id,
        connector_event_history.connector_type,
        connector_event_history.destination_name,
        connector_event_history.destination_id,
        max(connector_event_history.count_api_calls) as count_api_calls,
        max(connector_event_history.count_record_modifications) as count_record_modifications,
        max(connector_event_history.count_schema_changes) as count_schema_changes

    from
    spine left join connector_event_history
        on cast(spine.date_day as date) = connector_event_history.date_day

    group by 1,2,3,4,5,6
),

final as (

    select *
    from join_event_history

    where cast(date_day as timestamp) <= 
    current_timestamp


    order by date_day desc
)

select *
from final
  );
  
[0m16:31:41.123608 [debug] [Thread-18 ]: finished collecting timing info
[0m16:31:41.124468 [debug] [Thread-18 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b103913d-822b-4169-acb7-6fcb37cac0b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ba800d0>]}
[0m16:31:41.125062 [info ] [Thread-18 ]: 14 of 14 OK created table model dbt_workshop_20221013_dev_fivetran_log.fivetran_log__schema_changelog  [[32mCREATE TABLE (10.0 rows, 1.9 MB processed)[0m in 3.88s]
[0m16:31:41.125840 [debug] [Thread-18 ]: Finished running node model.fivetran_log.fivetran_log__schema_changelog
[0m16:31:44.270124 [debug] [Thread-16 ]: finished collecting timing info
[0m16:31:44.270995 [debug] [Thread-16 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b103913d-822b-4169-acb7-6fcb37cac0b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bcdb220>]}
[0m16:31:44.271423 [info ] [Thread-16 ]: 12 of 14 OK created incremental model dbt_workshop_20221013_dev_fivetran_log.fivetran_log__audit_table  [[32mCREATE TABLE (2.2k rows, 1.9 MB processed)[0m in 7.03s]
[0m16:31:44.272018 [debug] [Thread-16 ]: Finished running node model.fivetran_log.fivetran_log__audit_table
[0m16:31:44.891592 [debug] [Thread-17 ]: finished collecting timing info
[0m16:31:44.892570 [debug] [Thread-17 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b103913d-822b-4169-acb7-6fcb37cac0b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ba561c0>]}
[0m16:31:44.893106 [info ] [Thread-17 ]: 13 of 14 OK created table model dbt_workshop_20221013_dev_fivetran_log.fivetran_log__connector_daily_events  [[32mCREATE TABLE (93.0 rows, 1.9 MB processed)[0m in 7.65s]
[0m16:31:44.893745 [debug] [Thread-17 ]: Finished running node model.fivetran_log.fivetran_log__connector_daily_events
[0m16:31:44.897524 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m16:31:44.898447 [info ] [MainThread]: 
[0m16:31:44.898880 [info ] [MainThread]: Finished running 12 table models, 1 view model, 1 incremental model in 0 hours 0 minutes and 19.39 seconds (19.39s).
[0m16:31:44.899261 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:31:44.899460 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__account' was properly closed.
[0m16:31:44.899652 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__active_volume' was properly closed.
[0m16:31:44.899845 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__connector_tmp' was properly closed.
[0m16:31:44.900031 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__credits_used' was properly closed.
[0m16:31:44.900217 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__destination' was properly closed.
[0m16:31:44.900400 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__log' was properly closed.
[0m16:31:44.900660 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__usage_cost' was properly closed.
[0m16:31:44.900911 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__connector' was properly closed.
[0m16:31:44.901108 [debug] [MainThread]: Connection 'model.fivetran_log.fivetran_log__connector_status' was properly closed.
[0m16:31:44.901302 [debug] [MainThread]: Connection 'model.fivetran_log.fivetran_log__audit_table' was properly closed.
[0m16:31:44.901490 [debug] [MainThread]: Connection 'model.fivetran_log.fivetran_log__connector_daily_events' was properly closed.
[0m16:31:44.901682 [debug] [MainThread]: Connection 'model.fivetran_log.fivetran_log__schema_changelog' was properly closed.
[0m16:31:44.945672 [info ] [MainThread]: 
[0m16:31:44.946140 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:31:44.946740 [info ] [MainThread]: 
[0m16:31:44.947270 [error] [MainThread]: [33mRuntime Error in model stg_fivetran_log__active_volume (models/staging/stg_fivetran_log__active_volume.sql)[0m
[0m16:31:44.947774 [error] [MainThread]:   404 Not found: Table fivetran-hands-on-lab:fivetran_log.active_volume was not found in location US
[0m16:31:44.948358 [error] [MainThread]:   
[0m16:31:44.948835 [error] [MainThread]:   Location: US
[0m16:31:44.949200 [error] [MainThread]:   Job ID: b4b4cb0e-966e-44b2-a6fc-4de573b0bea4
[0m16:31:44.949546 [error] [MainThread]:   
[0m16:31:44.949929 [info ] [MainThread]: 
[0m16:31:44.950283 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=2 TOTAL=14
[0m16:31:44.950767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b61ac70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b613c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ba56190>]}


============================== 2022-10-13 16:36:01.146052 | 9a7b9681-8959-4896-be45-9a49cbcb4d9b ==============================
[0m16:36:01.146143 [info ] [MainThread]: Running with dbt=1.2.0
[0m16:36:01.147628 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/angel.hernandez/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['fivetran_log'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m16:36:01.147876 [debug] [MainThread]: Tracking: tracking
[0m16:36:01.174702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127922250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1279224c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1279220a0>]}
[0m16:36:01.339726 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:36:01.340019 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:36:01.350835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9a7b9681-8959-4896-be45-9a49cbcb4d9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127ccd0d0>]}
[0m16:36:01.374053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9a7b9681-8959-4896-be45-9a49cbcb4d9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127a0dd30>]}
[0m16:36:01.374443 [info ] [MainThread]: Found 16 models, 20 tests, 0 snapshots, 0 analyses, 622 macros, 0 operations, 0 seed files, 7 sources, 0 exposures, 0 metrics
[0m16:36:01.374828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a7b9681-8959-4896-be45-9a49cbcb4d9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127a0deb0>]}
[0m16:36:01.377468 [info ] [MainThread]: 
[0m16:36:01.378255 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m16:36:01.380303 [debug] [ThreadPool]: Acquiring new bigquery connection "list_fivetran-hands-on-lab"
[0m16:36:01.381043 [debug] [ThreadPool]: Acquiring new bigquery connection "list_fivetran-hands-on-lab"
[0m16:36:01.381351 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:36:01.381846 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:36:01.973121 [debug] [ThreadPool]: Acquiring new bigquery connection "list_fivetran-hands-on-lab_dbt_workshop_20221013_dev"
[0m16:36:01.973552 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:36:01.974232 [debug] [ThreadPool]: Acquiring new bigquery connection "list_fivetran-hands-on-lab_dbt_workshop_20221013_dev_stg_fivetran_log"
[0m16:36:01.975036 [debug] [ThreadPool]: Acquiring new bigquery connection "list_fivetran-hands-on-lab_dbt_workshop_20221013_dev_fivetran_log"
[0m16:36:01.975335 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:36:01.975709 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:36:02.304195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a7b9681-8959-4896-be45-9a49cbcb4d9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127cbabb0>]}
[0m16:36:02.305175 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m16:36:02.305645 [info ] [MainThread]: 
[0m16:36:02.315232 [debug] [Thread-1  ]: Began running node model.fivetran_log.stg_fivetran_log__account
[0m16:36:02.315518 [debug] [Thread-2  ]: Began running node model.fivetran_log.stg_fivetran_log__active_volume
[0m16:36:02.315779 [debug] [Thread-3  ]: Began running node model.fivetran_log.stg_fivetran_log__connector_tmp
[0m16:36:02.315992 [debug] [Thread-4  ]: Began running node model.fivetran_log.stg_fivetran_log__credits_used
[0m16:36:02.316312 [info ] [Thread-1  ]: 1 of 14 START table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__account  [RUN]
[0m16:36:02.316535 [debug] [Thread-5  ]: Began running node model.fivetran_log.stg_fivetran_log__destination
[0m16:36:02.316770 [debug] [Thread-6  ]: Began running node model.fivetran_log.stg_fivetran_log__log
[0m16:36:02.317028 [debug] [Thread-7  ]: Began running node model.fivetran_log.stg_fivetran_log__usage_cost
[0m16:36:02.317326 [info ] [Thread-2  ]: 2 of 14 START table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__active_volume  [RUN]
[0m16:36:02.317706 [info ] [Thread-3  ]: 3 of 14 START view model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__connector_tmp  [RUN]
[0m16:36:02.318064 [info ] [Thread-4  ]: 4 of 14 START table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__credits_used  [RUN]
[0m16:36:02.319145 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__account"
[0m16:36:02.319616 [info ] [Thread-5  ]: 5 of 14 START table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__destination  [RUN]
[0m16:36:02.320047 [info ] [Thread-6  ]: 6 of 14 START table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__log  [RUN]
[0m16:36:02.320424 [info ] [Thread-7  ]: 7 of 14 START table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__usage_cost  [RUN]
[0m16:36:02.321630 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__active_volume"
[0m16:36:02.322642 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__connector_tmp"
[0m16:36:02.323709 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__credits_used"
[0m16:36:02.324074 [debug] [Thread-1  ]: Began compiling node model.fivetran_log.stg_fivetran_log__account
[0m16:36:02.325318 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__destination"
[0m16:36:02.326302 [debug] [Thread-6  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__log"
[0m16:36:02.327521 [debug] [Thread-7  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__usage_cost"
[0m16:36:02.327823 [debug] [Thread-2  ]: Began compiling node model.fivetran_log.stg_fivetran_log__active_volume
[0m16:36:02.328082 [debug] [Thread-3  ]: Began compiling node model.fivetran_log.stg_fivetran_log__connector_tmp
[0m16:36:02.328328 [debug] [Thread-4  ]: Began compiling node model.fivetran_log.stg_fivetran_log__credits_used
[0m16:36:02.328602 [debug] [Thread-1  ]: Compiling model.fivetran_log.stg_fivetran_log__account
[0m16:36:02.328864 [debug] [Thread-5  ]: Began compiling node model.fivetran_log.stg_fivetran_log__destination
[0m16:36:02.329128 [debug] [Thread-6  ]: Began compiling node model.fivetran_log.stg_fivetran_log__log
[0m16:36:02.329373 [debug] [Thread-7  ]: Began compiling node model.fivetran_log.stg_fivetran_log__usage_cost
[0m16:36:02.329603 [debug] [Thread-2  ]: Compiling model.fivetran_log.stg_fivetran_log__active_volume
[0m16:36:02.329833 [debug] [Thread-3  ]: Compiling model.fivetran_log.stg_fivetran_log__connector_tmp
[0m16:36:02.330070 [debug] [Thread-4  ]: Compiling model.fivetran_log.stg_fivetran_log__credits_used
[0m16:36:02.340735 [debug] [Thread-1  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__account"
[0m16:36:02.341159 [debug] [Thread-5  ]: Compiling model.fivetran_log.stg_fivetran_log__destination
[0m16:36:02.341518 [debug] [Thread-6  ]: Compiling model.fivetran_log.stg_fivetran_log__log
[0m16:36:02.341808 [debug] [Thread-7  ]: Compiling model.fivetran_log.stg_fivetran_log__usage_cost
[0m16:36:02.349619 [debug] [Thread-3  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__connector_tmp"
[0m16:36:02.364375 [debug] [Thread-4  ]: On "model.fivetran_log.stg_fivetran_log__credits_used": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:36:02.371350 [debug] [Thread-5  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__destination"
[0m16:36:02.379157 [debug] [Thread-6  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__log"
[0m16:36:02.385347 [debug] [Thread-1  ]: finished collecting timing info
[0m16:36:02.386626 [debug] [Thread-7  ]: On "model.fivetran_log.stg_fivetran_log__usage_cost": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:36:02.387517 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m16:36:02.388279 [debug] [Thread-3  ]: finished collecting timing info
[0m16:36:02.388878 [debug] [Thread-5  ]: finished collecting timing info
[0m16:36:02.389112 [debug] [Thread-1  ]: Began executing node model.fivetran_log.stg_fivetran_log__account
[0m16:36:02.389746 [debug] [Thread-7  ]: Opening a new connection, currently in state init
[0m16:36:02.389978 [debug] [Thread-6  ]: finished collecting timing info
[0m16:36:02.390602 [debug] [Thread-3  ]: Began executing node model.fivetran_log.stg_fivetran_log__connector_tmp
[0m16:36:02.391177 [debug] [Thread-5  ]: Began executing node model.fivetran_log.stg_fivetran_log__destination
[0m16:36:02.416206 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:36:02.417149 [debug] [Thread-6  ]: Began executing node model.fivetran_log.stg_fivetran_log__log
[0m16:36:02.432723 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m16:36:02.478595 [debug] [Thread-3  ]: Writing runtime SQL for node "model.fivetran_log.stg_fivetran_log__connector_tmp"
[0m16:36:02.483589 [debug] [Thread-2  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__active_volume"
[0m16:36:02.488159 [debug] [Thread-6  ]: Opening a new connection, currently in state init
[0m16:36:02.490154 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m16:36:02.491348 [debug] [Thread-2  ]: finished collecting timing info
[0m16:36:02.492238 [debug] [Thread-2  ]: Began executing node model.fivetran_log.stg_fivetran_log__active_volume
[0m16:36:02.513077 [debug] [Thread-2  ]: Writing runtime SQL for node "model.fivetran_log.stg_fivetran_log__active_volume"
[0m16:36:02.515470 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m16:36:02.516105 [debug] [Thread-3  ]: On model.fivetran_log.stg_fivetran_log__connector_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__connector_tmp"} */


  create or replace view `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__connector_tmp`
  OPTIONS()
  as select *
from `fivetran-hands-on-lab`.`fivetran_log`.`connector`;


[0m16:36:02.524647 [debug] [Thread-2  ]: On model.fivetran_log.stg_fivetran_log__active_volume: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__active_volume"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__active_volume`
  
  
  OPTIONS()
  as (
    with active_volume as (

    select * from `fivetran-hands-on-lab`.`fivetran_log`.`active_volume`
),

fields as (

    select
        id as active_volume_id,
        connector_id as connector_name, -- Note: this misnomer will be changed by Fivetran soon
        destination_id,
        cast(measured_at as 
    timestamp
) as measured_at,
        monthly_active_rows,
        schema_name,
        table_name
    from active_volume
)

select * 
from fields
  );
  
[0m16:36:02.799644 [debug] [Thread-4  ]: On "model.fivetran_log.stg_fivetran_log__credits_used": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:36:02.802835 [debug] [Thread-1  ]: Writing runtime SQL for node "model.fivetran_log.stg_fivetran_log__account"
[0m16:36:02.805275 [debug] [Thread-1  ]: On model.fivetran_log.stg_fivetran_log__account: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__account"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__account`
  
  
  OPTIONS()
  as (
    with account as (
    
    select * 
    from `fivetran-hands-on-lab`.`fivetran_log`.`account`
),

fields as (

    select
        id as account_id,
        country,
        cast(created_at as 
    timestamp
) as created_at,
        name as account_name,
        status
    from account
)

select * 
from fields
  );
  
[0m16:36:02.923552 [debug] [Thread-5  ]: Writing runtime SQL for node "model.fivetran_log.stg_fivetran_log__destination"
[0m16:36:02.924221 [debug] [Thread-5  ]: On model.fivetran_log.stg_fivetran_log__destination: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__destination"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__destination`
  
  
  OPTIONS()
  as (
    with destination as (

    select * 
    from `fivetran-hands-on-lab`.`fivetran_log`.`destination`
),

fields as (

    select
        id as destination_id,
        account_id,
        cast(created_at as 
    timestamp
) as created_at,
        name as destination_name,
        region
    from destination
)

select * 
from fields
  );
  
[0m16:36:02.958002 [debug] [Thread-2  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__active_volume"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__active_volume`
  
  
  OPTIONS()
  as (
    with active_volume as (

    select * from `fivetran-hands-on-lab`.`fivetran_log`.`active_volume`
),

fields as (

    select
        id as active_volume_id,
        connector_id as connector_name, -- Note: this misnomer will be changed by Fivetran soon
        destination_id,
        cast(measured_at as 
    timestamp
) as measured_at,
        monthly_active_rows,
        schema_name,
        table_name
    from active_volume
)

select * 
from fields
  );
  
[0m16:36:02.958406 [debug] [Thread-2  ]: BigQuery adapter: 404 Not found: Table fivetran-hands-on-lab:fivetran_log.active_volume was not found in location US

Location: US
Job ID: 436a94e7-8f62-4582-9fe4-1bd628b9fae4

[0m16:36:02.958962 [debug] [Thread-2  ]: finished collecting timing info
[0m16:36:02.959902 [debug] [Thread-2  ]: Runtime Error in model stg_fivetran_log__active_volume (models/staging/stg_fivetran_log__active_volume.sql)
  404 Not found: Table fivetran-hands-on-lab:fivetran_log.active_volume was not found in location US
  
  Location: US
  Job ID: 436a94e7-8f62-4582-9fe4-1bd628b9fae4
  
[0m16:36:02.960308 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a7b9681-8959-4896-be45-9a49cbcb4d9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127dcc250>]}
[0m16:36:02.960823 [error] [Thread-2  ]: 2 of 14 ERROR creating table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__active_volume  [[31mERROR[0m in 0.64s]
[0m16:36:02.961548 [debug] [Thread-2  ]: Finished running node model.fivetran_log.stg_fivetran_log__active_volume
[0m16:36:02.984591 [debug] [Thread-7  ]: On "model.fivetran_log.stg_fivetran_log__usage_cost": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:36:02.992288 [debug] [Thread-6  ]: Writing runtime SQL for node "model.fivetran_log.stg_fivetran_log__log"
[0m16:36:02.993002 [debug] [Thread-6  ]: On model.fivetran_log.stg_fivetran_log__log: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__log"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__log`
  
  
  OPTIONS()
  as (
    with log as (

    select * 
    from `fivetran-hands-on-lab`.`fivetran_log`.`log`
),

fields as (

    select
        id as log_id, 
        cast(time_stamp as 
    timestamp
) as created_at,
        connector_id, -- Note: the connector_id column used to erroneously equal the connector_name, NOT its id.
        case when transformation_id is not null and event is null then 'TRANSFORMATION'
        else event end as event_type, 
        message_data,
        case 
        when transformation_id is not null and message_data like '%has succeeded%' then 'transformation run success'
        when transformation_id is not null and message_data like '%has failed%' then 'transformation run failed'
        else message_event end as event_subtype,
        transformation_id
    from log
)

select * 
from fields
  );
  
[0m16:36:03.062874 [debug] [Thread-4  ]: On "model.fivetran_log.stg_fivetran_log__credits_used": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:36:03.254744 [debug] [Thread-4  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__credits_used"
[0m16:36:03.257394 [debug] [Thread-4  ]: finished collecting timing info
[0m16:36:03.257659 [debug] [Thread-4  ]: Began executing node model.fivetran_log.stg_fivetran_log__credits_used
[0m16:36:03.290089 [debug] [Thread-7  ]: On "model.fivetran_log.stg_fivetran_log__usage_cost": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:36:03.526399 [debug] [Thread-4  ]: Writing runtime SQL for node "model.fivetran_log.stg_fivetran_log__credits_used"
[0m16:36:03.527208 [debug] [Thread-4  ]: On model.fivetran_log.stg_fivetran_log__credits_used: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__credits_used"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__credits_used`
  
  
  OPTIONS()
  as (
    

select
    cast(null as 
    string
) as destination_id,
    cast(null as 
    string
) as measured_month,
    cast(null as 
    int64
) as credits_spent


  );
  
[0m16:36:03.539542 [debug] [Thread-7  ]: On "model.fivetran_log.stg_fivetran_log__usage_cost": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:36:03.648344 [debug] [Thread-3  ]: finished collecting timing info
[0m16:36:03.649054 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a7b9681-8959-4896-be45-9a49cbcb4d9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127d519a0>]}
[0m16:36:03.649462 [info ] [Thread-3  ]: 3 of 14 OK created view model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__connector_tmp  [[32mOK[0m in 1.33s]
[0m16:36:03.649957 [debug] [Thread-3  ]: Finished running node model.fivetran_log.stg_fivetran_log__connector_tmp
[0m16:36:03.650421 [debug] [Thread-9  ]: Began running node model.fivetran_log.stg_fivetran_log__connector
[0m16:36:03.650748 [info ] [Thread-9  ]: 8 of 14 START table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__connector  [RUN]
[0m16:36:03.651582 [debug] [Thread-9  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__connector"
[0m16:36:03.651797 [debug] [Thread-9  ]: Began compiling node model.fivetran_log.stg_fivetran_log__connector
[0m16:36:03.651989 [debug] [Thread-9  ]: Compiling model.fivetran_log.stg_fivetran_log__connector
[0m16:36:03.657543 [debug] [Thread-9  ]: Opening a new connection, currently in state init
[0m16:36:03.737234 [debug] [Thread-7  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__usage_cost"
[0m16:36:03.739886 [debug] [Thread-7  ]: finished collecting timing info
[0m16:36:03.740547 [debug] [Thread-7  ]: Began executing node model.fivetran_log.stg_fivetran_log__usage_cost
[0m16:36:04.020148 [debug] [Thread-9  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__connector"
[0m16:36:04.021725 [debug] [Thread-7  ]: Writing runtime SQL for node "model.fivetran_log.stg_fivetran_log__usage_cost"
[0m16:36:04.022323 [debug] [Thread-9  ]: finished collecting timing info
[0m16:36:04.022546 [debug] [Thread-9  ]: Began executing node model.fivetran_log.stg_fivetran_log__connector
[0m16:36:04.026679 [debug] [Thread-7  ]: On model.fivetran_log.stg_fivetran_log__usage_cost: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__usage_cost"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__usage_cost`
  
  
  OPTIONS()
  as (
    

select
    cast(null as 
    string
) as destination_id,
    cast(null as 
    string
) as measured_month,
    cast(null as 
    int64
) as dollars_spent


  );
  
[0m16:36:04.246889 [debug] [Thread-9  ]: Writing runtime SQL for node "model.fivetran_log.stg_fivetran_log__connector"
[0m16:36:04.247655 [debug] [Thread-9  ]: On model.fivetran_log.stg_fivetran_log__connector: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.stg_fivetran_log__connector"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__connector`
  
  
  OPTIONS()
  as (
    with connector as (

    select * 
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__connector_tmp`
),

fields as (
    select
        
    
    
    _fivetran_deleted
    
 as 
    
    _fivetran_deleted
    
, 
    
    
    _fivetran_synced
    
 as 
    
    _fivetran_synced
    
, 
    
    
    connecting_user_id
    
 as 
    
    connecting_user_id
    
, 
    
    
    connector_id
    
 as 
    
    connector_id
    
, 
    
    
    connector_name
    
 as 
    
    connector_name
    
, 
    cast(null as 
    string
) as 
    
    connector_type
    
 , 
    
    
    connector_type_id
    
 as 
    
    connector_type_id
    
, 
    
    
    destination_id
    
 as 
    
    destination_id
    
, 
    
    
    paused
    
 as 
    
    paused
    
, 
    cast(null as 
    int64
) as 
    
    service_version
    
 , 
    
    
    signed_up
    
 as 
    
    signed_up
    



        ,row_number() over ( partition by connector_name, destination_id order by _fivetran_synced desc ) as nth_last_record
    from connector
),

final as (

    select 
        connector_id,
        connector_name,
        coalesce(connector_type_id, connector_type) as connector_type,
        destination_id,
        connecting_user_id,
        paused as is_paused,
        signed_up as set_up_at
    from fields

    -- Only look at the most recent one
    where nth_last_record = 1
)

select * 
from final
  );
  
[0m16:36:05.449878 [debug] [Thread-5  ]: finished collecting timing info
[0m16:36:05.450643 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a7b9681-8959-4896-be45-9a49cbcb4d9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127d51d30>]}
[0m16:36:05.451172 [info ] [Thread-5  ]: 5 of 14 OK created table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__destination  [[32mCREATE TABLE (1.0 rows, 68.0 Bytes processed)[0m in 3.13s]
[0m16:36:05.451781 [debug] [Thread-5  ]: Finished running node model.fivetran_log.stg_fivetran_log__destination
[0m16:36:05.753425 [debug] [Thread-1  ]: finished collecting timing info
[0m16:36:05.754161 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a7b9681-8959-4896-be45-9a49cbcb4d9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127d4cd00>]}
[0m16:36:05.754580 [info ] [Thread-1  ]: 1 of 14 OK created table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__account  [[32mCREATE TABLE (1.0 rows, 47.0 Bytes processed)[0m in 3.44s]
[0m16:36:05.755132 [debug] [Thread-1  ]: Finished running node model.fivetran_log.stg_fivetran_log__account
[0m16:36:05.889956 [debug] [Thread-6  ]: finished collecting timing info
[0m16:36:05.890864 [debug] [Thread-6  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a7b9681-8959-4896-be45-9a49cbcb4d9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127df4070>]}
[0m16:36:05.891385 [info ] [Thread-6  ]: 6 of 14 OK created table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__log  [[32mCREATE TABLE (14.5k rows, 2.4 MB processed)[0m in 3.56s]
[0m16:36:05.892078 [debug] [Thread-6  ]: Finished running node model.fivetran_log.stg_fivetran_log__log
[0m16:36:06.325350 [debug] [Thread-4  ]: finished collecting timing info
[0m16:36:06.326089 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a7b9681-8959-4896-be45-9a49cbcb4d9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127d51a60>]}
[0m16:36:06.326505 [info ] [Thread-4  ]: 4 of 14 OK created table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__credits_used  [[32mCREATE TABLE (1.0 rows, 0 processed)[0m in 4.00s]
[0m16:36:06.327056 [debug] [Thread-4  ]: Finished running node model.fivetran_log.stg_fivetran_log__credits_used
[0m16:36:06.494589 [debug] [Thread-7  ]: finished collecting timing info
[0m16:36:06.495459 [debug] [Thread-7  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a7b9681-8959-4896-be45-9a49cbcb4d9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127d51eb0>]}
[0m16:36:06.495918 [info ] [Thread-7  ]: 7 of 14 OK created table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__usage_cost  [[32mCREATE TABLE (1.0 rows, 0 processed)[0m in 4.17s]
[0m16:36:06.496446 [debug] [Thread-7  ]: Finished running node model.fivetran_log.stg_fivetran_log__usage_cost
[0m16:36:07.133408 [debug] [Thread-9  ]: finished collecting timing info
[0m16:36:07.134295 [debug] [Thread-9  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a7b9681-8959-4896-be45-9a49cbcb4d9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127f60520>]}
[0m16:36:07.134783 [info ] [Thread-9  ]: 8 of 14 OK created table model dbt_workshop_20221013_dev_stg_fivetran_log.stg_fivetran_log__connector  [[32mCREATE TABLE (1.0 rows, 98.0 Bytes processed)[0m in 3.48s]
[0m16:36:07.135468 [debug] [Thread-9  ]: Finished running node model.fivetran_log.stg_fivetran_log__connector
[0m16:36:07.136305 [debug] [Thread-11 ]: Began running node model.fivetran_log.fivetran_log__connector_status
[0m16:36:07.136781 [info ] [Thread-11 ]: 9 of 14 START table model dbt_workshop_20221013_dev_fivetran_log.fivetran_log__connector_status  [RUN]
[0m16:36:07.137065 [debug] [Thread-12 ]: Began running node model.fivetran_log.fivetran_log__mar_table_history
[0m16:36:07.137932 [debug] [Thread-11 ]: Acquiring new bigquery connection "model.fivetran_log.fivetran_log__connector_status"
[0m16:36:07.138256 [info ] [Thread-12 ]: 10 of 14 SKIP relation dbt_workshop_20221013_dev_fivetran_log.fivetran_log__mar_table_history  [[33mSKIP[0m]
[0m16:36:07.138536 [debug] [Thread-11 ]: Began compiling node model.fivetran_log.fivetran_log__connector_status
[0m16:36:07.139162 [debug] [Thread-12 ]: Finished running node model.fivetran_log.fivetran_log__mar_table_history
[0m16:36:07.139413 [debug] [Thread-11 ]: Compiling model.fivetran_log.fivetran_log__connector_status
[0m16:36:07.145288 [debug] [Thread-14 ]: Began running node model.fivetran_log.fivetran_log__usage_mar_destination_history
[0m16:36:07.169325 [info ] [Thread-14 ]: 11 of 14 SKIP relation dbt_workshop_20221013_dev_fivetran_log.fivetran_log__usage_mar_destination_history  [[33mSKIP[0m]
[0m16:36:07.186733 [debug] [Thread-11 ]: Writing injected SQL for node "model.fivetran_log.fivetran_log__connector_status"
[0m16:36:07.187502 [debug] [Thread-14 ]: Finished running node model.fivetran_log.fivetran_log__usage_mar_destination_history
[0m16:36:07.188326 [debug] [Thread-11 ]: finished collecting timing info
[0m16:36:07.188583 [debug] [Thread-11 ]: Began executing node model.fivetran_log.fivetran_log__connector_status
[0m16:36:07.192547 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m16:36:07.598493 [debug] [Thread-11 ]: Writing runtime SQL for node "model.fivetran_log.fivetran_log__connector_status"
[0m16:36:07.599354 [debug] [Thread-11 ]: On model.fivetran_log.fivetran_log__connector_status: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.fivetran_log__connector_status"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.`fivetran_log__connector_status`
  
  
  OPTIONS()
  as (
    with transformation_removal as (

    select *
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__log`
    where transformation_id is null

),

connector_log as (
    select 
        *,
        sum( case when event_subtype in ('sync_start') then 1 else 0 end) over ( partition by connector_id 
            order by created_at rows unbounded preceding) as sync_batch_id
    from transformation_removal
    -- only looking at errors, warnings, and syncs here
    where event_type = 'SEVERE'
        or event_type = 'WARNING'
        or event_subtype like 'sync%'
        or (event_subtype = 'status' 
            and 

 
  json_extract_scalar(message_data, '$.status')

 = 'RESCHEDULED'
            
            and 

 
  json_extract_scalar(message_data, '$.reason')

 like '%intended behavior%'
            ) -- for priority-first syncs. these should be captured by event_type = 'WARNING' but let's make sure
        or (event_subtype = 'status' 
            and 

 
  json_extract_scalar(message_data, '$.status')

 = 'SUCCESSFUL'
        )
        -- whole reason is "We have rescheduled the connector to force flush data from the forward sync into your destination. This is intended behavior and means that the connector is working as expected."
),

schema_changes as (

    select
        connector_id,
        count(*) as number_of_schema_changes_last_month

    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__log`

    where 
        datetime_diff(
        cast(
    current_timestamp
 as datetime),
        cast(created_at as datetime),
        day
    ) <= 30
        and event_subtype in ('create_table', 'alter_table', 'create_schema', 'change_schema_config')

    group by 1

),

connector as (

    select *
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__connector`

),

destination as (

    select * 
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__destination`
),

connector_metrics as (

    select
        connector.connector_id,
        connector.connector_name,
        connector.connector_type,
        connector.destination_id,
        connector.is_paused,
        connector.set_up_at,
        max(case when connector_log.event_subtype = 'sync_start' then connector_log.created_at else null end) as last_sync_started_at,

        max(case when connector_log.event_subtype = 'sync_end' 
            then connector_log.created_at else null end) as last_sync_completed_at,

        max(case when connector_log.event_subtype in ('status', 'sync_end')
                and 

 
  json_extract_scalar(connector_log.message_data, '$.status')

 ='SUCCESSFUL'
            then connector_log.created_at else null end) as last_successful_sync_completed_at,


        max(case when connector_log.event_subtype = 'sync_end' 
            then connector_log.sync_batch_id else null end) as last_sync_batch_id,

        max(case when connector_log.event_subtype in ('status', 'sync_end')
                and 

 
  json_extract_scalar(connector_log.message_data, '$.status')

 ='RESCHEDULED'
                and 

 
  json_extract_scalar(connector_log.message_data, '$.reason')

 like '%intended behavior%'
            then connector_log.created_at else null end) as last_priority_first_sync_completed_at,
                

        max(case when connector_log.event_type = 'SEVERE' then connector_log.created_at else null end) as last_error_at,

        max(case when connector_log.event_type = 'SEVERE' then connector_log.sync_batch_id else null end) as last_error_batch,
        max(case when event_type = 'WARNING' then connector_log.created_at else null end) as last_warning_at

    from connector 
    left join connector_log 
        on connector_log.connector_id = connector.connector_id
    group by 1,2,3,4,5,6

),

connector_health as (

    select
        *,
        case 
            -- connector is paused
            when is_paused then 'paused'

            -- a sync has never been attempted
            when last_sync_started_at is null then 'incomplete'

            -- a priority-first sync has occurred, but a normal sync has not
            when last_priority_first_sync_completed_at is not null and last_sync_completed_at is null then 'priority first sync'

            -- a priority sync has occurred more recently than a normal one (may occurr if the connector has been paused and resumed)
            when last_priority_first_sync_completed_at > last_sync_completed_at then 'priority first sync'

            -- a sync has been attempted, but not completed, and it's not due to errors. also a priority-first sync hasn't
            when last_sync_completed_at is null and last_error_at is null then 'initial sync in progress'

            -- the last attempted sync had an error
            when last_sync_batch_id = last_error_batch then 'broken'

            -- there's never been a successful sync and there have been errors
            when last_sync_completed_at is null and last_error_at is not null then 'broken'

        else 'connected' end as connector_health

    from connector_metrics
),

-- Joining with log to grab pertinent error/warning messagees
connector_recent_logs as (

    select 
        connector_health.connector_id,
        connector_health.connector_name,
        connector_health.connector_type,
        connector_health.destination_id,
        connector_health.connector_health,
        connector_health.last_successful_sync_completed_at,
        connector_health.last_sync_started_at,
        connector_health.last_sync_completed_at,
        connector_health.set_up_at,
        connector_log.event_subtype,
        connector_log.event_type,
        connector_log.message_data

    from connector_health 
    left join connector_log 
        on connector_log.connector_id = connector_health.connector_id
        -- limiting relevance to since the last successful sync completion (if there has been one)
        and connector_log.created_at > coalesce(connector_health.last_sync_completed_at, connector_health.last_priority_first_sync_completed_at, '2000-01-01') 
        -- only looking at errors and warnings (excluding syncs - both normal and priority first)
        and connector_log.event_type != 'INFO' 
        -- need to explicitly avoid priority first statuses because they are of event_type WARNING
        and not (connector_log.event_subtype = 'status' 
            and 

 
  json_extract_scalar(connector_log.message_data, '$.status')

 ='RESCHEDULED'
            and 

 
  json_extract_scalar(connector_log.message_data, '$.reason')

 like '%intended behavior%')

    group by 1,2,3,4,5,6,7,8,9,10,11,12 -- de-duping error messages
    

),

final as (

    select
        connector_recent_logs.connector_id,
        connector_recent_logs.connector_name,
        connector_recent_logs.connector_type,
        connector_recent_logs.destination_id,
        destination.destination_name,
        connector_recent_logs.connector_health,
        connector_recent_logs.last_successful_sync_completed_at,
        connector_recent_logs.last_sync_started_at,
        connector_recent_logs.last_sync_completed_at,
        connector_recent_logs.set_up_at,
        coalesce(schema_changes.number_of_schema_changes_last_month, 0) as number_of_schema_changes_last_month
        
        
        , 
    string_agg(distinct case when connector_recent_logs.event_type = 'SEVERE' then connector_recent_logs.message_data else null end, '\n')

 as errors_since_last_completed_sync
        , 
    string_agg(distinct case when connector_recent_logs.event_type = 'WARNING' then connector_recent_logs.message_data else null end, '\n')

 as warnings_since_last_completed_sync
        

    from connector_recent_logs
    left join schema_changes 
        on connector_recent_logs.connector_id = schema_changes.connector_id 

    join destination on destination.destination_id = connector_recent_logs.destination_id
    group by 1,2,3,4,5,6,7,8,9,10,11
)

select * from final
  );
  
[0m16:36:11.961758 [debug] [Thread-11 ]: finished collecting timing info
[0m16:36:11.962638 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a7b9681-8959-4896-be45-9a49cbcb4d9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127fec8e0>]}
[0m16:36:11.963141 [info ] [Thread-11 ]: 9 of 14 OK created table model dbt_workshop_20221013_dev_fivetran_log.fivetran_log__connector_status  [[32mCREATE TABLE (1.0 rows, 2.0 MB processed)[0m in 4.83s]
[0m16:36:11.963797 [debug] [Thread-11 ]: Finished running node model.fivetran_log.fivetran_log__connector_status
[0m16:36:11.964521 [debug] [Thread-16 ]: Began running node model.fivetran_log.fivetran_log__audit_table
[0m16:36:11.964791 [debug] [Thread-17 ]: Began running node model.fivetran_log.fivetran_log__connector_daily_events
[0m16:36:11.965067 [info ] [Thread-16 ]: 12 of 14 START incremental model dbt_workshop_20221013_dev_fivetran_log.fivetran_log__audit_table  [RUN]
[0m16:36:11.965260 [debug] [Thread-18 ]: Began running node model.fivetran_log.fivetran_log__schema_changelog
[0m16:36:11.965699 [info ] [Thread-17 ]: 13 of 14 START table model dbt_workshop_20221013_dev_fivetran_log.fivetran_log__connector_daily_events  [RUN]
[0m16:36:11.966571 [debug] [Thread-16 ]: Acquiring new bigquery connection "model.fivetran_log.fivetran_log__audit_table"
[0m16:36:11.966978 [info ] [Thread-18 ]: 14 of 14 START table model dbt_workshop_20221013_dev_fivetran_log.fivetran_log__schema_changelog  [RUN]
[0m16:36:11.967935 [debug] [Thread-17 ]: Acquiring new bigquery connection "model.fivetran_log.fivetran_log__connector_daily_events"
[0m16:36:11.968137 [debug] [Thread-16 ]: Began compiling node model.fivetran_log.fivetran_log__audit_table
[0m16:36:11.968945 [debug] [Thread-18 ]: Acquiring new bigquery connection "model.fivetran_log.fivetran_log__schema_changelog"
[0m16:36:11.969180 [debug] [Thread-17 ]: Began compiling node model.fivetran_log.fivetran_log__connector_daily_events
[0m16:36:11.969399 [debug] [Thread-16 ]: Compiling model.fivetran_log.fivetran_log__audit_table
[0m16:36:11.969649 [debug] [Thread-18 ]: Began compiling node model.fivetran_log.fivetran_log__schema_changelog
[0m16:36:11.969889 [debug] [Thread-17 ]: Compiling model.fivetran_log.fivetran_log__connector_daily_events
[0m16:36:11.981627 [debug] [Thread-18 ]: Compiling model.fivetran_log.fivetran_log__schema_changelog
[0m16:36:11.995575 [debug] [Thread-16 ]: Opening a new connection, currently in state init
[0m16:36:12.099653 [debug] [Thread-18 ]: Writing injected SQL for node "model.fivetran_log.fivetran_log__schema_changelog"
[0m16:36:12.103987 [debug] [Thread-17 ]: Opening a new connection, currently in state init
[0m16:36:12.104925 [debug] [Thread-18 ]: finished collecting timing info
[0m16:36:12.105230 [debug] [Thread-18 ]: Began executing node model.fivetran_log.fivetran_log__schema_changelog
[0m16:36:12.109050 [debug] [Thread-18 ]: Opening a new connection, currently in state init
[0m16:36:12.110856 [debug] [Thread-16 ]: On model.fivetran_log.fivetran_log__audit_table: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.fivetran_log__audit_table"} */
select date(max(sync_start)) from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.`fivetran_log__audit_table`
[0m16:36:12.111976 [debug] [Thread-17 ]: On model.fivetran_log.fivetran_log__connector_daily_events: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.fivetran_log__connector_daily_events"} */

    
        select  min( signed_up ) as min_date from `fivetran-hands-on-lab`.`fivetran_log`.`connector`
    
  
[0m16:36:12.425119 [debug] [Thread-18 ]: Writing runtime SQL for node "model.fivetran_log.fivetran_log__schema_changelog"
[0m16:36:12.425912 [debug] [Thread-18 ]: On model.fivetran_log.fivetran_log__schema_changelog: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.fivetran_log__schema_changelog"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.`fivetran_log__schema_changelog`
  
  
  OPTIONS()
  as (
    with schema_changes as (

    select *
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__log`

    where event_subtype in ('create_table', 'alter_table', 'create_schema', 'change_schema_config')
),

connector as (

    select *
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.`fivetran_log__connector_status`
),

add_connector_info as (

    select 
        schema_changes.*,
        connector.connector_name,
        connector.destination_id,
        connector.destination_name

    from schema_changes join connector using(connector_id)
),

final as (

    select
        connector_id,
        connector_name,
        destination_id,
        destination_name,
        created_at,
        event_subtype,
        message_data,

        case 
        when event_subtype = 'alter_table' then 

 
  json_extract_scalar(message_data, '$.table')

 
        when event_subtype = 'create_table' then 

 
  json_extract_scalar(message_data, '$.name')

 
        else null end as table_name,

        case 
        when event_subtype = 'create_schema' or event_subtype = 'create_table' then 

 
  json_extract_scalar(message_data, '$.schema')

 
        else null end as schema_name
    
    from add_connector_info
)

select * from final
order by created_at desc, connector_id
  );
  
[0m16:36:13.703308 [debug] [Thread-17 ]: On model.fivetran_log.fivetran_log__connector_daily_events: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.fivetran_log__connector_daily_events"} */


        select datetime_diff(
        cast(

        datetime_add(
            cast( timestamp_trunc(
        cast(
    current_timestamp
 as timestamp),
        day
    ) as datetime),
        interval 1 week
        )

 as datetime),
        cast(cast('2022-07-13' as date) as datetime),
        day
    )
[0m16:36:14.765024 [debug] [Thread-16 ]: Writing injected SQL for node "model.fivetran_log.fivetran_log__audit_table"
[0m16:36:14.765622 [debug] [Thread-16 ]: finished collecting timing info
[0m16:36:14.765834 [debug] [Thread-16 ]: Began executing node model.fivetran_log.fivetran_log__audit_table
[0m16:36:15.084117 [debug] [Thread-17 ]: Writing injected SQL for node "model.fivetran_log.fivetran_log__connector_daily_events"
[0m16:36:15.084779 [debug] [Thread-17 ]: finished collecting timing info
[0m16:36:15.085018 [debug] [Thread-17 ]: Began executing node model.fivetran_log.fivetran_log__connector_daily_events
[0m16:36:15.120318 [debug] [Thread-16 ]: Writing runtime SQL for node "model.fivetran_log.fivetran_log__audit_table"
[0m16:36:15.158714 [debug] [Thread-16 ]: On model.fivetran_log.fivetran_log__audit_table: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.fivetran_log__audit_table"} */

        
            
            
        
    

    

    merge into `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.`fivetran_log__audit_table` as DBT_INTERNAL_DEST
        using (
          

with sync_log as (
    
    select 
        *,
        

 
  json_extract_scalar(message_data, '$.table')

 as table_name
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__log`
    where event_subtype in ('sync_start', 'sync_end', 'write_to_table_start', 'write_to_table_end', 'records_modified')

    

    -- Capture the latest timestamp in a call statement instead of a subquery for optimizing BQ costs on incremental runs-- load the result from the above query into a new variable-- the query_result is stored as a dataframe. Therefore, we want to now store it as a singular value.-- compare the new batch of data to the latest sync already stored in this model
    and date(created_at) >= '2022-10-12'

    
),


connector as (

    select *
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.`fivetran_log__connector_status`
),

add_connector_info as (

    select 
        sync_log.*,
        connector.connector_name,
        connector.destination_id,
        connector.destination_name
    from sync_log 
    left join connector
        on connector.connector_id = sync_log.connector_id
),

sync_timestamps as (

    select
        connector_id,
        connector_name,
        table_name,
        event_subtype,
        destination_id,
        destination_name,
        created_at as write_to_table_start,
        min(case when event_subtype = 'write_to_table_end' then created_at else null end) 
            over (partition by connector_id, table_name order by created_at ROWS between CURRENT ROW AND UNBOUNDED FOLLOWING) as write_to_table_end,

        max(case when event_subtype = 'sync_start' then created_at else null end) 
            over (partition by connector_id order by created_at ROWS between UNBOUNDED PRECEDING and CURRENT ROW) as sync_start,

        min(case when event_subtype = 'sync_end' then created_at else null end) 
            over (partition by connector_id order by created_at ROWS between CURRENT ROW AND UNBOUNDED FOLLOWING) as sync_end, -- coalesce with next_sync_start

        min(case when event_subtype = 'sync_start' then created_at else null end) 
            over (partition by connector_id order by created_at ROWS between CURRENT ROW AND UNBOUNDED FOLLOWING) as next_sync_start
    from add_connector_info
),

-- this will be the base for every record in the final CTE
limit_to_table_starts as (

    select *
    from sync_timestamps 
    where event_subtype = 'write_to_table_start'
),

records_modified_log as (

    select 
        connector_id,
        created_at,
        

 
  json_extract_scalar(message_data, '$.table')

 as table_name,
        

 
  json_extract_scalar(message_data, '$.schema')

 as schema_name,
        

 
  json_extract_scalar(message_data, '$.operationType')

 as operation_type,
        cast (

 
  json_extract_scalar(message_data, '$.count')

 as 
    int64
) as row_count
    from sync_log 
    where event_subtype = 'records_modified'

),

sum_records_modified as (

    select
        limit_to_table_starts.connector_id,
        limit_to_table_starts.connector_name,
        limit_to_table_starts.table_name,
        limit_to_table_starts.destination_id,
        limit_to_table_starts.destination_name,
        limit_to_table_starts.write_to_table_start,
        limit_to_table_starts.write_to_table_end,
        limit_to_table_starts.sync_start,
        case when limit_to_table_starts.sync_end > limit_to_table_starts.next_sync_start then null else limit_to_table_starts.sync_end end as sync_end,
        sum(case when records_modified_log.operation_type = 'REPLACED_OR_INSERTED' then records_modified_log.row_count else 0 end) as sum_rows_replaced_or_inserted,
        sum(case when records_modified_log.operation_type = 'UPDATED' then records_modified_log.row_count else 0 end) as sum_rows_updated,
        sum(case when records_modified_log.operation_type = 'DELETED' then records_modified_log.row_count else 0 end) as sum_rows_deleted
    from limit_to_table_starts
    left join records_modified_log on 
        limit_to_table_starts.connector_id = records_modified_log.connector_id
        and limit_to_table_starts.table_name = records_modified_log.table_name

        -- confine it to one sync
        and records_modified_log.created_at > limit_to_table_starts.sync_start 
        and records_modified_log.created_at < coalesce(limit_to_table_starts.sync_end, limit_to_table_starts.next_sync_start) 

    group by 1,2,3,4,5,6,7,8,9
),

surrogate_key as (

    select 
        *,
        to_hex(md5(cast(coalesce(cast(connector_id as 
    string
), '') || '-' || coalesce(cast(destination_id as 
    string
), '') || '-' || coalesce(cast(table_name as 
    string
), '') || '-' || coalesce(cast(write_to_table_start as 
    string
), '') as 
    string
))) as unique_table_sync_key
    from sum_records_modified
)

select *
from surrogate_key
        ) as DBT_INTERNAL_SOURCE
        on 
                DBT_INTERNAL_SOURCE.unique_table_sync_key = DBT_INTERNAL_DEST.unique_table_sync_key
            

    
    when matched then update set
        `connector_id` = DBT_INTERNAL_SOURCE.`connector_id`,`connector_name` = DBT_INTERNAL_SOURCE.`connector_name`,`table_name` = DBT_INTERNAL_SOURCE.`table_name`,`destination_id` = DBT_INTERNAL_SOURCE.`destination_id`,`destination_name` = DBT_INTERNAL_SOURCE.`destination_name`,`write_to_table_start` = DBT_INTERNAL_SOURCE.`write_to_table_start`,`write_to_table_end` = DBT_INTERNAL_SOURCE.`write_to_table_end`,`sync_start` = DBT_INTERNAL_SOURCE.`sync_start`,`sync_end` = DBT_INTERNAL_SOURCE.`sync_end`,`sum_rows_replaced_or_inserted` = DBT_INTERNAL_SOURCE.`sum_rows_replaced_or_inserted`,`sum_rows_updated` = DBT_INTERNAL_SOURCE.`sum_rows_updated`,`sum_rows_deleted` = DBT_INTERNAL_SOURCE.`sum_rows_deleted`,`unique_table_sync_key` = DBT_INTERNAL_SOURCE.`unique_table_sync_key`
    

    when not matched then insert
        (`connector_id`, `connector_name`, `table_name`, `destination_id`, `destination_name`, `write_to_table_start`, `write_to_table_end`, `sync_start`, `sync_end`, `sum_rows_replaced_or_inserted`, `sum_rows_updated`, `sum_rows_deleted`, `unique_table_sync_key`)
    values
        (`connector_id`, `connector_name`, `table_name`, `destination_id`, `destination_name`, `write_to_table_start`, `write_to_table_end`, `sync_start`, `sync_end`, `sum_rows_replaced_or_inserted`, `sum_rows_updated`, `sum_rows_deleted`, `unique_table_sync_key`)


  
[0m16:36:15.310596 [debug] [Thread-18 ]: finished collecting timing info
[0m16:36:15.311366 [debug] [Thread-18 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a7b9681-8959-4896-be45-9a49cbcb4d9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127f62f10>]}
[0m16:36:15.311796 [info ] [Thread-18 ]: 14 of 14 OK created table model dbt_workshop_20221013_dev_fivetran_log.fivetran_log__schema_changelog  [[32mCREATE TABLE (10.0 rows, 1.9 MB processed)[0m in 3.34s]
[0m16:36:15.312385 [debug] [Thread-18 ]: Finished running node model.fivetran_log.fivetran_log__schema_changelog
[0m16:36:15.421571 [debug] [Thread-17 ]: Writing runtime SQL for node "model.fivetran_log.fivetran_log__connector_daily_events"
[0m16:36:15.422418 [debug] [Thread-17 ]: On model.fivetran_log.fivetran_log__connector_daily_events: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.fivetran_log__connector_daily_events"} */


  create or replace table `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.`fivetran_log__connector_daily_events`
  
  
  OPTIONS()
  as (
    -- depends_on: `fivetran-hands-on-lab`.`fivetran_log`.`connector`

with connector as (
    
    select * 
    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.`fivetran_log__connector_status`
),

-- grab api calls, schema changes, and record modifications
log_events as (

    select 
        connector_id,
        cast( timestamp_trunc(
        cast(created_at as timestamp),
        day
    ) as date) as date_day,
        case 
            when event_subtype in ('create_table', 'alter_table', 'create_schema', 'change_schema_config') then 'schema_change' 
            else event_subtype end as event_subtype,

        sum(case when event_subtype = 'records_modified' then cast( 

 
  json_extract_scalar(message_data, '$.count')

 as 
    int64
 )
        else 1 end) as count_events 

    from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.`stg_fivetran_log__log`

    where event_subtype in ('api_call', 
                            'records_modified', 
                            'create_table', 'alter_table', 'create_schema', 'change_schema_config') -- all schema changes
                            
        and connector_id is not null

    group by 1,2,3
),

pivot_out_events as (

    select
        connector_id,
        date_day,
        max(case when event_subtype = 'api_call' then count_events else 0 end) as count_api_calls,
        max(case when event_subtype = 'records_modified' then count_events else 0 end) as count_record_modifications,
        max(case when event_subtype = 'schema_change' then count_events else 0 end) as count_schema_changes

    from log_events
    group by 1,2
),

connector_event_counts as (

    select
        pivot_out_events.date_day,
        pivot_out_events.count_api_calls,
        pivot_out_events.count_record_modifications,
        pivot_out_events.count_schema_changes,
        connector.connector_name,
        connector.connector_id,
        connector.connector_type,
        connector.destination_name,
        connector.destination_id,
        connector.set_up_at
    from
    connector left join pivot_out_events 
        on pivot_out_events.connector_id = connector.connector_id
),

spine as (

    
    
    
    
    

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
    
    

    )

    select *
    from unioned
    where generated_number <= 99
    order by generated_number



),

all_periods as (

    select (
        

        datetime_add(
            cast( cast('2022-07-13' as date) as datetime),
        interval row_number() over (order by 1) - 1 day
        )


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= 

        datetime_add(
            cast( timestamp_trunc(
        cast(
    current_timestamp
 as timestamp),
        day
    ) as datetime),
        interval 1 week
        )



)

select * from filtered

 
),

connector_event_history as (

    select
        cast(spine.date_day as date) as date_day,
        connector_event_counts.connector_name,
        connector_event_counts.connector_id,
        connector_event_counts.connector_type,
        connector_event_counts.destination_name,
        connector_event_counts.destination_id,
        max(case 
            when cast(spine.date_day as date) = connector_event_counts.date_day then connector_event_counts.count_api_calls
            else 0
        end) as count_api_calls,
        max(case 
            when cast(spine.date_day as date) = connector_event_counts.date_day then connector_event_counts.count_record_modifications
            else 0
        end) as count_record_modifications,
        max(case 
            when cast(spine.date_day as date) = connector_event_counts.date_day then connector_event_counts.count_schema_changes
            else 0
        end) as count_schema_changes
    from
    spine join connector_event_counts
        on spine.date_day  >= cast( timestamp_trunc(
        cast(connector_event_counts.set_up_at as timestamp),
        day
    ) as date)

    group by 1,2,3,4,5,6
),

-- now rejoin spine to get a complete calendar
join_event_history as (
    
    select
        spine.date_day,
        connector_event_history.connector_name,
        connector_event_history.connector_id,
        connector_event_history.connector_type,
        connector_event_history.destination_name,
        connector_event_history.destination_id,
        max(connector_event_history.count_api_calls) as count_api_calls,
        max(connector_event_history.count_record_modifications) as count_record_modifications,
        max(connector_event_history.count_schema_changes) as count_schema_changes

    from
    spine left join connector_event_history
        on cast(spine.date_day as date) = connector_event_history.date_day

    group by 1,2,3,4,5,6
),

final as (

    select *
    from join_event_history

    where cast(date_day as timestamp) <= 
    current_timestamp


    order by date_day desc
)

select *
from final
  );
  
[0m16:36:19.597839 [debug] [Thread-17 ]: finished collecting timing info
[0m16:36:19.598736 [debug] [Thread-17 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a7b9681-8959-4896-be45-9a49cbcb4d9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127f62f70>]}
[0m16:36:19.599217 [info ] [Thread-17 ]: 13 of 14 OK created table model dbt_workshop_20221013_dev_fivetran_log.fivetran_log__connector_daily_events  [[32mCREATE TABLE (93.0 rows, 1.9 MB processed)[0m in 7.63s]
[0m16:36:19.599780 [debug] [Thread-17 ]: Finished running node model.fivetran_log.fivetran_log__connector_daily_events
[0m16:36:19.697013 [debug] [Thread-16 ]: finished collecting timing info
[0m16:36:19.697979 [debug] [Thread-16 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a7b9681-8959-4896-be45-9a49cbcb4d9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127f62130>]}
[0m16:36:19.698540 [info ] [Thread-16 ]: 12 of 14 OK created incremental model dbt_workshop_20221013_dev_fivetran_log.fivetran_log__audit_table  [[32mMERGE (8.0 rows, 2.3 MB processed)[0m in 7.73s]
[0m16:36:19.699215 [debug] [Thread-16 ]: Finished running node model.fivetran_log.fivetran_log__audit_table
[0m16:36:19.702217 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m16:36:19.703276 [info ] [MainThread]: 
[0m16:36:19.703814 [info ] [MainThread]: Finished running 12 table models, 1 view model, 1 incremental model in 0 hours 0 minutes and 18.33 seconds (18.33s).
[0m16:36:19.704226 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:36:19.704438 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__account' was properly closed.
[0m16:36:19.704630 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__active_volume' was properly closed.
[0m16:36:19.704832 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__connector_tmp' was properly closed.
[0m16:36:19.705008 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__credits_used' was properly closed.
[0m16:36:19.705186 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__destination' was properly closed.
[0m16:36:19.705360 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__log' was properly closed.
[0m16:36:19.705536 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__usage_cost' was properly closed.
[0m16:36:19.705710 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__connector' was properly closed.
[0m16:36:19.705881 [debug] [MainThread]: Connection 'model.fivetran_log.fivetran_log__connector_status' was properly closed.
[0m16:36:19.706051 [debug] [MainThread]: Connection 'model.fivetran_log.fivetran_log__audit_table' was properly closed.
[0m16:36:19.706222 [debug] [MainThread]: Connection 'model.fivetran_log.fivetran_log__connector_daily_events' was properly closed.
[0m16:36:19.706393 [debug] [MainThread]: Connection 'model.fivetran_log.fivetran_log__schema_changelog' was properly closed.
[0m16:36:19.733376 [info ] [MainThread]: 
[0m16:36:19.733851 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:36:19.734249 [info ] [MainThread]: 
[0m16:36:19.734614 [error] [MainThread]: [33mRuntime Error in model stg_fivetran_log__active_volume (models/staging/stg_fivetran_log__active_volume.sql)[0m
[0m16:36:19.735004 [error] [MainThread]:   404 Not found: Table fivetran-hands-on-lab:fivetran_log.active_volume was not found in location US
[0m16:36:19.735353 [error] [MainThread]:   
[0m16:36:19.735693 [error] [MainThread]:   Location: US
[0m16:36:19.736044 [error] [MainThread]:   Job ID: 436a94e7-8f62-4582-9fe4-1bd628b9fae4
[0m16:36:19.736380 [error] [MainThread]:   
[0m16:36:19.736964 [info ] [MainThread]: 
[0m16:36:19.737535 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=2 TOTAL=14
[0m16:36:19.738391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127962070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127caf700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127e53f40>]}


============================== 2022-10-13 16:39:18.835826 | 7aa84d24-a221-4447-b032-a08061a0b6ab ==============================
[0m16:39:18.835838 [info ] [MainThread]: Running with dbt=1.2.0
[0m16:39:18.838179 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/angel.hernandez/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'deps', 'rpc_method': 'deps', 'indirect_selection': 'eager'}
[0m16:39:18.838409 [debug] [MainThread]: Tracking: tracking
[0m16:39:18.867058 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb46f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb468b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb46970>]}
[0m16:39:18.870694 [debug] [MainThread]: Set downloads directory='/var/folders/mk/2ztnrxy17z11rwb071p5c8c80000gp/T/dbt-downloads-53sj3c85'
[0m16:39:18.871797 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m16:39:18.938831 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m16:39:18.939294 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_log.json
[0m16:39:18.977027 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_log.json 200
[0m16:39:18.982647 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json
[0m16:39:19.014821 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json 200
[0m16:39:19.033351 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/spark_utils.json
[0m16:39:19.071698 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/spark_utils.json 200
[0m16:39:19.075522 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m16:39:19.136154 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m16:39:19.233943 [info ] [MainThread]: Installing fivetran/fivetran_log
[0m16:39:19.666679 [info ] [MainThread]:   Installed from version 0.6.3
[0m16:39:19.667261 [info ] [MainThread]:   Up to date!
[0m16:39:19.667795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '7aa84d24-a221-4447-b032-a08061a0b6ab', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb04bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb52fa0>]}
[0m16:39:19.668205 [info ] [MainThread]: Installing fivetran/fivetran_utils
[0m16:39:19.969862 [info ] [MainThread]:   Installed from version 0.3.9
[0m16:39:19.970406 [info ] [MainThread]:   Updated version available: 0.4.0
[0m16:39:19.970848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '7aa84d24-a221-4447-b032-a08061a0b6ab', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb6abe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb6ad30>]}
[0m16:39:19.971169 [info ] [MainThread]: Installing dbt-labs/spark_utils
[0m16:39:20.172207 [info ] [MainThread]:   Installed from version 0.3.0
[0m16:39:20.172645 [info ] [MainThread]:   Up to date!
[0m16:39:20.173045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '7aa84d24-a221-4447-b032-a08061a0b6ab', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc16970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc16640>]}
[0m16:39:20.173344 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m16:39:20.668955 [info ] [MainThread]:   Installed from version 0.8.6
[0m16:39:20.669456 [info ] [MainThread]:   Updated version available: 0.9.2
[0m16:39:20.669839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '7aa84d24-a221-4447-b032-a08061a0b6ab', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc16820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc16760>]}
[0m16:39:20.670234 [info ] [MainThread]: 
[0m16:39:20.670687 [info ] [MainThread]: Updates available for packages: ['fivetran/fivetran_utils', 'dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
[0m16:39:20.673216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10badd670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba90370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb040d0>]}


============================== 2022-10-13 16:47:49.878835 | f6195fa8-d102-4d60-bd51-8f7fd7c7fa7a ==============================
[0m16:47:49.878886 [info ] [MainThread]: Running with dbt=1.2.0
[0m16:47:49.880250 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/angel.hernandez/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m16:47:49.880556 [debug] [MainThread]: Tracking: tracking
[0m16:47:49.909693 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12331dc70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12331d370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12331d340>]}
[0m16:47:50.130064 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:47:50.130385 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:47:50.141861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f6195fa8-d102-4d60-bd51-8f7fd7c7fa7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1236c70d0>]}
[0m16:47:50.165240 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f6195fa8-d102-4d60-bd51-8f7fd7c7fa7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123524100>]}
[0m16:47:50.165644 [info ] [MainThread]: Found 16 models, 20 tests, 0 snapshots, 0 analyses, 622 macros, 0 operations, 0 seed files, 7 sources, 0 exposures, 0 metrics
[0m16:47:50.166018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f6195fa8-d102-4d60-bd51-8f7fd7c7fa7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1233c7ee0>]}
[0m16:47:50.169870 [info ] [MainThread]: 
[0m16:47:50.170853 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m16:47:50.173008 [debug] [ThreadPool]: Acquiring new bigquery connection "list_fivetran-hands-on-lab_dbt_workshop_20221013_dev_stg_fivetran_log"
[0m16:47:50.173942 [debug] [ThreadPool]: Acquiring new bigquery connection "list_fivetran-hands-on-lab_dbt_workshop_20221013_dev"
[0m16:47:50.174217 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:47:50.175140 [debug] [ThreadPool]: Acquiring new bigquery connection "list_fivetran-hands-on-lab_dbt_workshop_20221013_dev_fivetran_log"
[0m16:47:50.175471 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:47:50.175914 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:47:50.507217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f6195fa8-d102-4d60-bd51-8f7fd7c7fa7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123379ac0>]}
[0m16:47:50.508334 [info ] [MainThread]: Concurrency: 32 threads (target='dev')
[0m16:47:50.508888 [info ] [MainThread]: 
[0m16:47:50.533537 [debug] [Thread-1  ]: Began running node model.dbt_workshop_20221013.my_first_dbt_model
[0m16:47:50.534541 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_workshop_20221013.my_first_dbt_model"
[0m16:47:50.534929 [debug] [Thread-2  ]: Began running node model.fivetran_log.stg_fivetran_log__account
[0m16:47:50.535256 [debug] [Thread-1  ]: Began compiling node model.dbt_workshop_20221013.my_first_dbt_model
[0m16:47:50.538760 [debug] [Thread-1  ]: Compiling model.dbt_workshop_20221013.my_first_dbt_model
[0m16:47:50.535562 [debug] [Thread-3  ]: Began running node model.fivetran_log.stg_fivetran_log__active_volume
[0m16:47:50.536194 [debug] [Thread-5  ]: Began running node model.fivetran_log.stg_fivetran_log__credits_used
[0m16:47:50.545653 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__credits_used"
[0m16:47:50.545883 [debug] [Thread-5  ]: Began compiling node model.fivetran_log.stg_fivetran_log__credits_used
[0m16:47:50.537198 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__account"
[0m16:47:50.543743 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_workshop_20221013.my_first_dbt_model"
[0m16:47:50.544159 [debug] [Thread-8  ]: Began running node model.fivetran_log.stg_fivetran_log__usage_cost
[0m16:47:50.535879 [debug] [Thread-4  ]: Began running node model.fivetran_log.stg_fivetran_log__connector_tmp
[0m16:47:50.547965 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__connector_tmp"
[0m16:47:50.548202 [debug] [Thread-4  ]: Began compiling node model.fivetran_log.stg_fivetran_log__connector_tmp
[0m16:47:50.548415 [debug] [Thread-4  ]: Compiling model.fivetran_log.stg_fivetran_log__connector_tmp
[0m16:47:50.544948 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__active_volume"
[0m16:47:50.538275 [debug] [Thread-7  ]: Began running node model.fivetran_log.stg_fivetran_log__log
[0m16:47:50.547208 [debug] [Thread-8  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__usage_cost"
[0m16:47:50.546097 [debug] [Thread-5  ]: Compiling model.fivetran_log.stg_fivetran_log__credits_used
[0m16:47:50.546378 [debug] [Thread-2  ]: Began compiling node model.fivetran_log.stg_fivetran_log__account
[0m16:47:50.537865 [debug] [Thread-6  ]: Began running node model.fivetran_log.stg_fivetran_log__destination
[0m16:47:50.554859 [debug] [Thread-4  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__connector_tmp"
[0m16:47:50.555121 [debug] [Thread-3  ]: Began compiling node model.fivetran_log.stg_fivetran_log__active_volume
[0m16:47:50.556712 [debug] [Thread-8  ]: Began compiling node model.fivetran_log.stg_fivetran_log__usage_cost
[0m16:47:50.557005 [debug] [Thread-1  ]: finished collecting timing info
[0m16:47:50.569441 [debug] [Thread-2  ]: Compiling model.fivetran_log.stg_fivetran_log__account
[0m16:47:50.572006 [debug] [Thread-5  ]: On "model.fivetran_log.stg_fivetran_log__credits_used": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:47:50.572681 [debug] [Thread-6  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__destination"
[0m16:47:50.573794 [debug] [Thread-3  ]: Compiling model.fivetran_log.stg_fivetran_log__active_volume
[0m16:47:50.574785 [debug] [Thread-8  ]: Compiling model.fivetran_log.stg_fivetran_log__usage_cost
[0m16:47:50.575409 [debug] [Thread-4  ]: finished collecting timing info
[0m16:47:50.575667 [debug] [Thread-1  ]: Began executing node model.dbt_workshop_20221013.my_first_dbt_model
[0m16:47:50.582336 [debug] [Thread-7  ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__log"
[0m16:47:50.585974 [debug] [Thread-2  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__account"
[0m16:47:50.586244 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m16:47:50.586573 [debug] [Thread-6  ]: Began compiling node model.fivetran_log.stg_fivetran_log__destination
[0m16:47:50.607783 [debug] [Thread-8  ]: On "model.fivetran_log.stg_fivetran_log__usage_cost": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:47:50.607379 [debug] [Thread-3  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__active_volume"
[0m16:47:50.608129 [debug] [Thread-4  ]: Began executing node model.fivetran_log.stg_fivetran_log__connector_tmp
[0m16:47:50.608407 [debug] [Thread-1  ]: finished collecting timing info
[0m16:47:50.608620 [debug] [Thread-7  ]: Began compiling node model.fivetran_log.stg_fivetran_log__log
[0m16:47:50.609230 [debug] [Thread-6  ]: Compiling model.fivetran_log.stg_fivetran_log__destination
[0m16:47:50.609413 [debug] [Thread-2  ]: finished collecting timing info
[0m16:47:50.609730 [debug] [Thread-8  ]: Opening a new connection, currently in state init
[0m16:47:50.610114 [debug] [Thread-4  ]: finished collecting timing info
[0m16:47:50.610866 [debug] [Thread-1  ]: Finished running node model.dbt_workshop_20221013.my_first_dbt_model
[0m16:47:50.611150 [debug] [Thread-7  ]: Compiling model.fivetran_log.stg_fivetran_log__log
[0m16:47:50.611327 [debug] [Thread-3  ]: finished collecting timing info
[0m16:47:50.617154 [debug] [Thread-6  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__destination"
[0m16:47:50.617405 [debug] [Thread-2  ]: Began executing node model.fivetran_log.stg_fivetran_log__account
[0m16:47:50.618310 [debug] [Thread-4  ]: Finished running node model.fivetran_log.stg_fivetran_log__connector_tmp
[0m16:47:50.625571 [debug] [Thread-10 ]: Began running node model.dbt_workshop_20221013.my_second_dbt_model
[0m16:47:50.625870 [debug] [Thread-11 ]: Began running node test.dbt_workshop_20221013.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:47:50.626926 [debug] [Thread-7  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__log"
[0m16:47:50.627167 [debug] [Thread-12 ]: Began running node test.dbt_workshop_20221013.unique_my_first_dbt_model_id.16e066b321
[0m16:47:50.627369 [debug] [Thread-3  ]: Began executing node model.fivetran_log.stg_fivetran_log__active_volume
[0m16:47:50.627990 [debug] [Thread-2  ]: finished collecting timing info
[0m16:47:50.628986 [debug] [Thread-6  ]: finished collecting timing info
[0m16:47:50.629782 [debug] [Thread-10 ]: Acquiring new bigquery connection "model.dbt_workshop_20221013.my_second_dbt_model"
[0m16:47:50.629980 [debug] [Thread-14 ]: Began running node model.fivetran_log.stg_fivetran_log__connector
[0m16:47:50.630928 [debug] [Thread-11 ]: Acquiring new bigquery connection "test.dbt_workshop_20221013.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:47:50.631619 [debug] [Thread-12 ]: Acquiring new bigquery connection "test.dbt_workshop_20221013.unique_my_first_dbt_model_id.16e066b321"
[0m16:47:50.631999 [debug] [Thread-3  ]: finished collecting timing info
[0m16:47:50.632702 [debug] [Thread-2  ]: Finished running node model.fivetran_log.stg_fivetran_log__account
[0m16:47:50.632877 [debug] [Thread-7  ]: finished collecting timing info
[0m16:47:50.633141 [debug] [Thread-6  ]: Began executing node model.fivetran_log.stg_fivetran_log__destination
[0m16:47:50.633963 [debug] [Thread-10 ]: Began compiling node model.dbt_workshop_20221013.my_second_dbt_model
[0m16:47:50.634541 [debug] [Thread-14 ]: Acquiring new bigquery connection "model.fivetran_log.stg_fivetran_log__connector"
[0m16:47:50.635313 [debug] [Thread-11 ]: Began compiling node test.dbt_workshop_20221013.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:47:50.635719 [debug] [Thread-12 ]: Began compiling node test.dbt_workshop_20221013.unique_my_first_dbt_model_id.16e066b321
[0m16:47:50.636640 [debug] [Thread-3  ]: Finished running node model.fivetran_log.stg_fivetran_log__active_volume
[0m16:47:50.637785 [debug] [Thread-16 ]: Began running node test.fivetran_log.not_null_stg_fivetran_log__account_account_id.19a03e662a
[0m16:47:50.637967 [debug] [Thread-7  ]: Began executing node model.fivetran_log.stg_fivetran_log__log
[0m16:47:50.638164 [debug] [Thread-17 ]: Began running node test.fivetran_log.unique_stg_fivetran_log__account_account_id.3de58e95dc
[0m16:47:50.638776 [debug] [Thread-6  ]: finished collecting timing info
[0m16:47:50.638988 [debug] [Thread-10 ]: Compiling model.dbt_workshop_20221013.my_second_dbt_model
[0m16:47:50.639203 [debug] [Thread-14 ]: Began compiling node model.fivetran_log.stg_fivetran_log__connector
[0m16:47:50.639423 [debug] [Thread-11 ]: Compiling test.dbt_workshop_20221013.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:47:50.639671 [debug] [Thread-12 ]: Compiling test.dbt_workshop_20221013.unique_my_first_dbt_model_id.16e066b321
[0m16:47:50.640157 [debug] [Thread-19 ]: Began running node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__active_volume_active_volume_id__destination_id.d6532db276
[0m16:47:50.640723 [debug] [Thread-16 ]: Acquiring new bigquery connection "test.fivetran_log.not_null_stg_fivetran_log__account_account_id.19a03e662a"
[0m16:47:50.640945 [debug] [Thread-7  ]: finished collecting timing info
[0m16:47:50.641509 [debug] [Thread-17 ]: Acquiring new bigquery connection "test.fivetran_log.unique_stg_fivetran_log__account_account_id.3de58e95dc"
[0m16:47:50.642196 [debug] [Thread-6  ]: Finished running node model.fivetran_log.stg_fivetran_log__destination
[0m16:47:50.647558 [debug] [Thread-10 ]: Writing injected SQL for node "model.dbt_workshop_20221013.my_second_dbt_model"
[0m16:47:50.647966 [debug] [Thread-14 ]: Compiling model.fivetran_log.stg_fivetran_log__connector
[0m16:47:50.667956 [debug] [Thread-12 ]: Writing injected SQL for node "test.dbt_workshop_20221013.unique_my_first_dbt_model_id.16e066b321"
[0m16:47:50.673575 [debug] [Thread-11 ]: Writing injected SQL for node "test.dbt_workshop_20221013.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:47:50.674116 [debug] [Thread-19 ]: Acquiring new bigquery connection "test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__active_volume_active_volume_id__destination_id.d6532db276"
[0m16:47:50.674398 [debug] [Thread-16 ]: Began compiling node test.fivetran_log.not_null_stg_fivetran_log__account_account_id.19a03e662a
[0m16:47:50.675122 [debug] [Thread-7  ]: Finished running node model.fivetran_log.stg_fivetran_log__log
[0m16:47:50.675618 [debug] [Thread-17 ]: Began compiling node test.fivetran_log.unique_stg_fivetran_log__account_account_id.3de58e95dc
[0m16:47:50.676328 [debug] [Thread-21 ]: Began running node test.fivetran_log.not_null_stg_fivetran_log__destination_destination_id.bf0be221ed
[0m16:47:50.676626 [debug] [Thread-22 ]: Began running node test.fivetran_log.unique_stg_fivetran_log__destination_destination_id.f88b94a8cb
[0m16:47:50.681766 [debug] [Thread-14 ]: Opening a new connection, currently in state init
[0m16:47:50.682793 [debug] [Thread-10 ]: finished collecting timing info
[0m16:47:50.683368 [debug] [Thread-19 ]: Began compiling node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__active_volume_active_volume_id__destination_id.d6532db276
[0m16:47:50.683759 [debug] [Thread-16 ]: Compiling test.fivetran_log.not_null_stg_fivetran_log__account_account_id.19a03e662a
[0m16:47:50.684044 [debug] [Thread-12 ]: finished collecting timing info
[0m16:47:50.684204 [debug] [Thread-11 ]: finished collecting timing info
[0m16:47:50.684619 [debug] [Thread-17 ]: Compiling test.fivetran_log.unique_stg_fivetran_log__account_account_id.3de58e95dc
[0m16:47:50.685213 [debug] [Thread-21 ]: Acquiring new bigquery connection "test.fivetran_log.not_null_stg_fivetran_log__destination_destination_id.bf0be221ed"
[0m16:47:50.685412 [debug] [Thread-24 ]: Began running node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__log_log_id__created_at.b6392465d9
[0m16:47:50.685979 [debug] [Thread-22 ]: Acquiring new bigquery connection "test.fivetran_log.unique_stg_fivetran_log__destination_destination_id.f88b94a8cb"
[0m16:47:50.686302 [debug] [Thread-10 ]: Began executing node model.dbt_workshop_20221013.my_second_dbt_model
[0m16:47:50.686577 [debug] [Thread-19 ]: Compiling test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__active_volume_active_volume_id__destination_id.d6532db276
[0m16:47:50.692539 [debug] [Thread-16 ]: Writing injected SQL for node "test.fivetran_log.not_null_stg_fivetran_log__account_account_id.19a03e662a"
[0m16:47:50.692862 [debug] [Thread-12 ]: Began executing node test.dbt_workshop_20221013.unique_my_first_dbt_model_id.16e066b321
[0m16:47:50.693077 [debug] [Thread-11 ]: Began executing node test.dbt_workshop_20221013.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:47:50.700331 [debug] [Thread-17 ]: Writing injected SQL for node "test.fivetran_log.unique_stg_fivetran_log__account_account_id.3de58e95dc"
[0m16:47:50.700640 [debug] [Thread-21 ]: Began compiling node test.fivetran_log.not_null_stg_fivetran_log__destination_destination_id.bf0be221ed
[0m16:47:50.701834 [debug] [Thread-24 ]: Acquiring new bigquery connection "test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__log_log_id__created_at.b6392465d9"
[0m16:47:50.702058 [debug] [Thread-22 ]: Began compiling node test.fivetran_log.unique_stg_fivetran_log__destination_destination_id.f88b94a8cb
[0m16:47:50.702411 [debug] [Thread-10 ]: finished collecting timing info
[0m16:47:50.718931 [debug] [Thread-19 ]: Writing injected SQL for node "test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__active_volume_active_volume_id__destination_id.d6532db276"
[0m16:47:50.720357 [debug] [Thread-12 ]: finished collecting timing info
[0m16:47:50.721725 [debug] [Thread-11 ]: finished collecting timing info
[0m16:47:50.724584 [debug] [Thread-16 ]: finished collecting timing info
[0m16:47:50.724766 [debug] [Thread-21 ]: Compiling test.fivetran_log.not_null_stg_fivetran_log__destination_destination_id.bf0be221ed
[0m16:47:50.725550 [debug] [Thread-24 ]: Began compiling node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__log_log_id__created_at.b6392465d9
[0m16:47:50.725984 [debug] [Thread-17 ]: finished collecting timing info
[0m16:47:50.726480 [debug] [Thread-22 ]: Compiling test.fivetran_log.unique_stg_fivetran_log__destination_destination_id.f88b94a8cb
[0m16:47:50.727489 [debug] [Thread-10 ]: Finished running node model.dbt_workshop_20221013.my_second_dbt_model
[0m16:47:50.728303 [debug] [Thread-12 ]: Finished running node test.dbt_workshop_20221013.unique_my_first_dbt_model_id.16e066b321
[0m16:47:50.729013 [debug] [Thread-11 ]: Finished running node test.dbt_workshop_20221013.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:47:50.729260 [debug] [Thread-16 ]: Began executing node test.fivetran_log.not_null_stg_fivetran_log__account_account_id.19a03e662a
[0m16:47:50.729453 [debug] [Thread-19 ]: finished collecting timing info
[0m16:47:50.735645 [debug] [Thread-21 ]: Writing injected SQL for node "test.fivetran_log.not_null_stg_fivetran_log__destination_destination_id.bf0be221ed"
[0m16:47:50.735979 [debug] [Thread-24 ]: Compiling test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__log_log_id__created_at.b6392465d9
[0m16:47:50.736392 [debug] [Thread-17 ]: Began executing node test.fivetran_log.unique_stg_fivetran_log__account_account_id.3de58e95dc
[0m16:47:50.742029 [debug] [Thread-22 ]: Writing injected SQL for node "test.fivetran_log.unique_stg_fivetran_log__destination_destination_id.f88b94a8cb"
[0m16:47:50.742862 [debug] [Thread-16 ]: finished collecting timing info
[0m16:47:50.743114 [debug] [Thread-26 ]: Began running node test.dbt_workshop_20221013.not_null_my_second_dbt_model_id.151b76d778
[0m16:47:50.743342 [debug] [Thread-27 ]: Began running node test.dbt_workshop_20221013.unique_my_second_dbt_model_id.57a0f8c493
[0m16:47:50.743537 [debug] [Thread-19 ]: Began executing node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__active_volume_active_volume_id__destination_id.d6532db276
[0m16:47:50.751332 [debug] [Thread-24 ]: Writing injected SQL for node "test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__log_log_id__created_at.b6392465d9"
[0m16:47:50.751723 [debug] [Thread-17 ]: finished collecting timing info
[0m16:47:50.751930 [debug] [Thread-21 ]: finished collecting timing info
[0m16:47:50.752704 [debug] [Thread-16 ]: Finished running node test.fivetran_log.not_null_stg_fivetran_log__account_account_id.19a03e662a
[0m16:47:50.753265 [debug] [Thread-26 ]: Acquiring new bigquery connection "test.dbt_workshop_20221013.not_null_my_second_dbt_model_id.151b76d778"
[0m16:47:50.753881 [debug] [Thread-27 ]: Acquiring new bigquery connection "test.dbt_workshop_20221013.unique_my_second_dbt_model_id.57a0f8c493"
[0m16:47:50.754129 [debug] [Thread-22 ]: finished collecting timing info
[0m16:47:50.754295 [debug] [Thread-19 ]: finished collecting timing info
[0m16:47:50.755039 [debug] [Thread-17 ]: Finished running node test.fivetran_log.unique_stg_fivetran_log__account_account_id.3de58e95dc
[0m16:47:50.755265 [debug] [Thread-21 ]: Began executing node test.fivetran_log.not_null_stg_fivetran_log__destination_destination_id.bf0be221ed
[0m16:47:50.755700 [debug] [Thread-24 ]: finished collecting timing info
[0m16:47:50.755906 [debug] [Thread-26 ]: Began compiling node test.dbt_workshop_20221013.not_null_my_second_dbt_model_id.151b76d778
[0m16:47:50.756099 [debug] [Thread-27 ]: Began compiling node test.dbt_workshop_20221013.unique_my_second_dbt_model_id.57a0f8c493
[0m16:47:50.756290 [debug] [Thread-22 ]: Began executing node test.fivetran_log.unique_stg_fivetran_log__destination_destination_id.f88b94a8cb
[0m16:47:50.756908 [debug] [Thread-19 ]: Finished running node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__active_volume_active_volume_id__destination_id.d6532db276
[0m16:47:50.757258 [debug] [Thread-21 ]: finished collecting timing info
[0m16:47:50.757469 [debug] [Thread-24 ]: Began executing node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__log_log_id__created_at.b6392465d9
[0m16:47:50.757672 [debug] [Thread-26 ]: Compiling test.dbt_workshop_20221013.not_null_my_second_dbt_model_id.151b76d778
[0m16:47:50.757861 [debug] [Thread-27 ]: Compiling test.dbt_workshop_20221013.unique_my_second_dbt_model_id.57a0f8c493
[0m16:47:50.758068 [debug] [Thread-22 ]: finished collecting timing info
[0m16:47:50.758858 [debug] [Thread-21 ]: Finished running node test.fivetran_log.not_null_stg_fivetran_log__destination_destination_id.bf0be221ed
[0m16:47:50.759073 [debug] [Thread-24 ]: finished collecting timing info
[0m16:47:50.820972 [debug] [Thread-26 ]: Writing injected SQL for node "test.dbt_workshop_20221013.not_null_my_second_dbt_model_id.151b76d778"
[0m16:47:50.828590 [debug] [Thread-27 ]: Writing injected SQL for node "test.dbt_workshop_20221013.unique_my_second_dbt_model_id.57a0f8c493"
[0m16:47:50.829612 [debug] [Thread-22 ]: Finished running node test.fivetran_log.unique_stg_fivetran_log__destination_destination_id.f88b94a8cb
[0m16:47:50.830654 [debug] [Thread-24 ]: Finished running node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__log_log_id__created_at.b6392465d9
[0m16:47:50.831425 [debug] [Thread-26 ]: finished collecting timing info
[0m16:47:50.831707 [debug] [Thread-26 ]: Began executing node test.dbt_workshop_20221013.not_null_my_second_dbt_model_id.151b76d778
[0m16:47:50.831901 [debug] [Thread-27 ]: finished collecting timing info
[0m16:47:50.832137 [debug] [Thread-26 ]: finished collecting timing info
[0m16:47:50.832368 [debug] [Thread-27 ]: Began executing node test.dbt_workshop_20221013.unique_my_second_dbt_model_id.57a0f8c493
[0m16:47:50.833089 [debug] [Thread-26 ]: Finished running node test.dbt_workshop_20221013.not_null_my_second_dbt_model_id.151b76d778
[0m16:47:50.833329 [debug] [Thread-27 ]: finished collecting timing info
[0m16:47:50.834179 [debug] [Thread-27 ]: Finished running node test.dbt_workshop_20221013.unique_my_second_dbt_model_id.57a0f8c493
[0m16:47:50.976299 [debug] [Thread-5  ]: On "model.fivetran_log.stg_fivetran_log__credits_used": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:47:51.116992 [debug] [Thread-8  ]: On "model.fivetran_log.stg_fivetran_log__usage_cost": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:47:51.216174 [debug] [Thread-14 ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__connector"
[0m16:47:51.216955 [debug] [Thread-14 ]: finished collecting timing info
[0m16:47:51.217304 [debug] [Thread-14 ]: Began executing node model.fivetran_log.stg_fivetran_log__connector
[0m16:47:51.217580 [debug] [Thread-14 ]: finished collecting timing info
[0m16:47:51.218624 [debug] [Thread-14 ]: Finished running node model.fivetran_log.stg_fivetran_log__connector
[0m16:47:51.219974 [debug] [Thread-29 ]: Began running node model.fivetran_log.fivetran_log__connector_status
[0m16:47:51.220393 [debug] [Thread-30 ]: Began running node model.fivetran_log.fivetran_log__mar_table_history
[0m16:47:51.221353 [debug] [Thread-29 ]: Acquiring new bigquery connection "model.fivetran_log.fivetran_log__connector_status"
[0m16:47:51.221655 [debug] [Thread-31 ]: Began running node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__connector_connector_name__destination_id.c6898547d8
[0m16:47:51.222453 [debug] [Thread-30 ]: Acquiring new bigquery connection "model.fivetran_log.fivetran_log__mar_table_history"
[0m16:47:51.222788 [debug] [Thread-29 ]: Began compiling node model.fivetran_log.fivetran_log__connector_status
[0m16:47:51.223622 [debug] [Thread-31 ]: Acquiring new bigquery connection "test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__connector_connector_name__destination_id.c6898547d8"
[0m16:47:51.223937 [debug] [Thread-30 ]: Began compiling node model.fivetran_log.fivetran_log__mar_table_history
[0m16:47:51.224194 [debug] [Thread-29 ]: Compiling model.fivetran_log.fivetran_log__connector_status
[0m16:47:51.224457 [debug] [Thread-31 ]: Began compiling node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__connector_connector_name__destination_id.c6898547d8
[0m16:47:51.224717 [debug] [Thread-30 ]: Compiling model.fivetran_log.fivetran_log__mar_table_history
[0m16:47:51.235958 [debug] [Thread-31 ]: Compiling test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__connector_connector_name__destination_id.c6898547d8
[0m16:47:51.268806 [debug] [Thread-30 ]: Writing injected SQL for node "model.fivetran_log.fivetran_log__mar_table_history"
[0m16:47:51.276428 [debug] [Thread-29 ]: Writing injected SQL for node "model.fivetran_log.fivetran_log__connector_status"
[0m16:47:51.287417 [debug] [Thread-31 ]: Writing injected SQL for node "test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__connector_connector_name__destination_id.c6898547d8"
[0m16:47:51.287992 [debug] [Thread-5  ]: On "model.fivetran_log.stg_fivetran_log__credits_used": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:47:51.288429 [debug] [Thread-30 ]: finished collecting timing info
[0m16:47:51.289795 [debug] [Thread-30 ]: Began executing node model.fivetran_log.fivetran_log__mar_table_history
[0m16:47:51.290036 [debug] [Thread-29 ]: finished collecting timing info
[0m16:47:51.290468 [debug] [Thread-31 ]: finished collecting timing info
[0m16:47:51.290669 [debug] [Thread-30 ]: finished collecting timing info
[0m16:47:51.291178 [debug] [Thread-29 ]: Began executing node model.fivetran_log.fivetran_log__connector_status
[0m16:47:51.291663 [debug] [Thread-31 ]: Began executing node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__connector_connector_name__destination_id.c6898547d8
[0m16:47:51.292715 [debug] [Thread-30 ]: Finished running node model.fivetran_log.fivetran_log__mar_table_history
[0m16:47:51.293152 [debug] [Thread-29 ]: finished collecting timing info
[0m16:47:51.293645 [debug] [Thread-31 ]: finished collecting timing info
[0m16:47:51.294347 [debug] [Thread-9  ]: Began running node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__mar_table_history_connector_id__destination_id__table_name__measured_month.2e5ecfcbac
[0m16:47:51.295146 [debug] [Thread-29 ]: Finished running node model.fivetran_log.fivetran_log__connector_status
[0m16:47:51.296230 [debug] [Thread-31 ]: Finished running node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__connector_connector_name__destination_id.c6898547d8
[0m16:47:51.296855 [debug] [Thread-9  ]: Acquiring new bigquery connection "test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__mar_table_history_connector_id__destination_id__table_name__measured_month.2e5ecfcbac"
[0m16:47:51.297434 [debug] [Thread-13 ]: Began running node model.fivetran_log.fivetran_log__audit_table
[0m16:47:51.297673 [debug] [Thread-4  ]: Began running node model.fivetran_log.fivetran_log__connector_daily_events
[0m16:47:51.297908 [debug] [Thread-31 ]: Began running node model.fivetran_log.fivetran_log__schema_changelog
[0m16:47:51.298105 [debug] [Thread-15 ]: Began running node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__connector_status_connector_id__destination_id.a2135f04e0
[0m16:47:51.298421 [debug] [Thread-9  ]: Began compiling node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__mar_table_history_connector_id__destination_id__table_name__measured_month.2e5ecfcbac
[0m16:47:51.299062 [debug] [Thread-13 ]: Acquiring new bigquery connection "model.fivetran_log.fivetran_log__audit_table"
[0m16:47:51.299646 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.fivetran_log.fivetran_log__connector_daily_events"
[0m16:47:51.300233 [debug] [Thread-31 ]: Acquiring new bigquery connection "model.fivetran_log.fivetran_log__schema_changelog"
[0m16:47:51.300812 [debug] [Thread-15 ]: Acquiring new bigquery connection "test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__connector_status_connector_id__destination_id.a2135f04e0"
[0m16:47:51.301034 [debug] [Thread-9  ]: Compiling test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__mar_table_history_connector_id__destination_id__table_name__measured_month.2e5ecfcbac
[0m16:47:51.301247 [debug] [Thread-13 ]: Began compiling node model.fivetran_log.fivetran_log__audit_table
[0m16:47:51.301464 [debug] [Thread-4  ]: Began compiling node model.fivetran_log.fivetran_log__connector_daily_events
[0m16:47:51.301677 [debug] [Thread-31 ]: Began compiling node model.fivetran_log.fivetran_log__schema_changelog
[0m16:47:51.301900 [debug] [Thread-15 ]: Began compiling node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__connector_status_connector_id__destination_id.a2135f04e0
[0m16:47:51.311327 [debug] [Thread-9  ]: Writing injected SQL for node "test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__mar_table_history_connector_id__destination_id__table_name__measured_month.2e5ecfcbac"
[0m16:47:51.311581 [debug] [Thread-13 ]: Compiling model.fivetran_log.fivetran_log__audit_table
[0m16:47:51.311846 [debug] [Thread-4  ]: Compiling model.fivetran_log.fivetran_log__connector_daily_events
[0m16:47:51.312093 [debug] [Thread-31 ]: Compiling model.fivetran_log.fivetran_log__schema_changelog
[0m16:47:51.312322 [debug] [Thread-15 ]: Compiling test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__connector_status_connector_id__destination_id.a2135f04e0
[0m16:47:51.335195 [debug] [Thread-9  ]: finished collecting timing info
[0m16:47:51.351988 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m16:47:51.357368 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m16:47:51.365672 [debug] [Thread-31 ]: Writing injected SQL for node "model.fivetran_log.fivetran_log__schema_changelog"
[0m16:47:51.373100 [debug] [Thread-15 ]: Writing injected SQL for node "test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__connector_status_connector_id__destination_id.a2135f04e0"
[0m16:47:51.373300 [debug] [Thread-9  ]: Began executing node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__mar_table_history_connector_id__destination_id__table_name__measured_month.2e5ecfcbac
[0m16:47:51.374462 [debug] [Thread-9  ]: finished collecting timing info
[0m16:47:51.375179 [debug] [Thread-9  ]: Finished running node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__mar_table_history_connector_id__destination_id__table_name__measured_month.2e5ecfcbac
[0m16:47:51.375759 [debug] [Thread-31 ]: finished collecting timing info
[0m16:47:51.375978 [debug] [Thread-15 ]: finished collecting timing info
[0m16:47:51.376184 [debug] [Thread-31 ]: Began executing node model.fivetran_log.fivetran_log__schema_changelog
[0m16:47:51.376392 [debug] [Thread-15 ]: Began executing node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__connector_status_connector_id__destination_id.a2135f04e0
[0m16:47:51.376604 [debug] [Thread-31 ]: finished collecting timing info
[0m16:47:51.376802 [debug] [Thread-15 ]: finished collecting timing info
[0m16:47:51.377492 [debug] [Thread-31 ]: Finished running node model.fivetran_log.fivetran_log__schema_changelog
[0m16:47:51.378508 [debug] [Thread-15 ]: Finished running node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__connector_status_connector_id__destination_id.a2135f04e0
[0m16:47:51.379261 [debug] [Thread-18 ]: Began running node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__schema_changelog_connector_id__destination_id__message_data__created_at.542291325d
[0m16:47:51.379881 [debug] [Thread-18 ]: Acquiring new bigquery connection "test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__schema_changelog_connector_id__destination_id__message_data__created_at.542291325d"
[0m16:47:51.380198 [debug] [Thread-4  ]: On model.fivetran_log.fivetran_log__connector_daily_events: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.fivetran_log__connector_daily_events"} */

    
        select  min( signed_up ) as min_date from `fivetran-hands-on-lab`.`fivetran_log`.`connector`
    
  
[0m16:47:51.380376 [debug] [Thread-18 ]: Began compiling node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__schema_changelog_connector_id__destination_id__message_data__created_at.542291325d
[0m16:47:51.380604 [debug] [Thread-13 ]: On model.fivetran_log.fivetran_log__audit_table: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.fivetran_log__audit_table"} */
select date(max(sync_start)) from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.`fivetran_log__audit_table`
[0m16:47:51.382014 [debug] [Thread-18 ]: Compiling test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__schema_changelog_connector_id__destination_id__message_data__created_at.542291325d
[0m16:47:51.393460 [debug] [Thread-18 ]: Writing injected SQL for node "test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__schema_changelog_connector_id__destination_id__message_data__created_at.542291325d"
[0m16:47:51.396470 [debug] [Thread-18 ]: finished collecting timing info
[0m16:47:51.397455 [debug] [Thread-18 ]: Began executing node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__schema_changelog_connector_id__destination_id__message_data__created_at.542291325d
[0m16:47:51.397744 [debug] [Thread-18 ]: finished collecting timing info
[0m16:47:51.398532 [debug] [Thread-18 ]: Finished running node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__schema_changelog_connector_id__destination_id__message_data__created_at.542291325d
[0m16:47:51.464522 [debug] [Thread-5  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__credits_used"
[0m16:47:51.467298 [debug] [Thread-5  ]: finished collecting timing info
[0m16:47:51.467657 [debug] [Thread-5  ]: Began executing node model.fivetran_log.stg_fivetran_log__credits_used
[0m16:47:51.468418 [debug] [Thread-5  ]: finished collecting timing info
[0m16:47:51.470148 [debug] [Thread-5  ]: Finished running node model.fivetran_log.stg_fivetran_log__credits_used
[0m16:47:51.471077 [debug] [Thread-20 ]: Began running node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__credits_used_measured_month__destination_id.ad5114f878
[0m16:47:51.472331 [debug] [Thread-20 ]: Acquiring new bigquery connection "test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__credits_used_measured_month__destination_id.ad5114f878"
[0m16:47:51.473531 [debug] [Thread-20 ]: Began compiling node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__credits_used_measured_month__destination_id.ad5114f878
[0m16:47:51.473782 [debug] [Thread-20 ]: Compiling test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__credits_used_measured_month__destination_id.ad5114f878
[0m16:47:51.484041 [debug] [Thread-20 ]: Writing injected SQL for node "test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__credits_used_measured_month__destination_id.ad5114f878"
[0m16:47:51.486509 [debug] [Thread-8  ]: On "model.fivetran_log.stg_fivetran_log__usage_cost": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:47:51.487984 [debug] [Thread-20 ]: finished collecting timing info
[0m16:47:51.488281 [debug] [Thread-20 ]: Began executing node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__credits_used_measured_month__destination_id.ad5114f878
[0m16:47:51.488513 [debug] [Thread-20 ]: finished collecting timing info
[0m16:47:51.489581 [debug] [Thread-20 ]: Finished running node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__credits_used_measured_month__destination_id.ad5114f878
[0m16:47:51.672155 [debug] [Thread-8  ]: On "model.fivetran_log.stg_fivetran_log__usage_cost": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:47:51.904463 [debug] [Thread-8  ]: Writing injected SQL for node "model.fivetran_log.stg_fivetran_log__usage_cost"
[0m16:47:51.905150 [debug] [Thread-8  ]: finished collecting timing info
[0m16:47:51.905395 [debug] [Thread-8  ]: Began executing node model.fivetran_log.stg_fivetran_log__usage_cost
[0m16:47:51.905615 [debug] [Thread-8  ]: finished collecting timing info
[0m16:47:51.906534 [debug] [Thread-8  ]: Finished running node model.fivetran_log.stg_fivetran_log__usage_cost
[0m16:47:51.907064 [debug] [Thread-23 ]: Began running node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__usage_cost_measured_month__destination_id.3759bdc054
[0m16:47:51.907356 [debug] [Thread-7  ]: Began running node model.fivetran_log.fivetran_log__usage_mar_destination_history
[0m16:47:51.908051 [debug] [Thread-23 ]: Acquiring new bigquery connection "test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__usage_cost_measured_month__destination_id.3759bdc054"
[0m16:47:51.908706 [debug] [Thread-7  ]: Acquiring new bigquery connection "model.fivetran_log.fivetran_log__usage_mar_destination_history"
[0m16:47:51.908949 [debug] [Thread-23 ]: Began compiling node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__usage_cost_measured_month__destination_id.3759bdc054
[0m16:47:51.909169 [debug] [Thread-7  ]: Began compiling node model.fivetran_log.fivetran_log__usage_mar_destination_history
[0m16:47:51.909418 [debug] [Thread-23 ]: Compiling test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__usage_cost_measured_month__destination_id.3759bdc054
[0m16:47:51.909652 [debug] [Thread-7  ]: Compiling model.fivetran_log.fivetran_log__usage_mar_destination_history
[0m16:47:51.919326 [debug] [Thread-23 ]: Writing injected SQL for node "test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__usage_cost_measured_month__destination_id.3759bdc054"
[0m16:47:51.935904 [debug] [Thread-7  ]: Writing injected SQL for node "model.fivetran_log.fivetran_log__usage_mar_destination_history"
[0m16:47:51.936452 [debug] [Thread-23 ]: finished collecting timing info
[0m16:47:51.936713 [debug] [Thread-23 ]: Began executing node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__usage_cost_measured_month__destination_id.3759bdc054
[0m16:47:51.936973 [debug] [Thread-23 ]: finished collecting timing info
[0m16:47:51.937833 [debug] [Thread-23 ]: Finished running node test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__usage_cost_measured_month__destination_id.3759bdc054
[0m16:47:51.938290 [debug] [Thread-7  ]: finished collecting timing info
[0m16:47:51.938540 [debug] [Thread-7  ]: Began executing node model.fivetran_log.fivetran_log__usage_mar_destination_history
[0m16:47:51.938760 [debug] [Thread-7  ]: finished collecting timing info
[0m16:47:51.939543 [debug] [Thread-7  ]: Finished running node model.fivetran_log.fivetran_log__usage_mar_destination_history
[0m16:47:51.940010 [debug] [Thread-10 ]: Began running node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__usage_mar_destination_history_destination_id__measured_month.fa03d71c0b
[0m16:47:51.940645 [debug] [Thread-10 ]: Acquiring new bigquery connection "test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__usage_mar_destination_history_destination_id__measured_month.fa03d71c0b"
[0m16:47:51.940869 [debug] [Thread-10 ]: Began compiling node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__usage_mar_destination_history_destination_id__measured_month.fa03d71c0b
[0m16:47:51.941075 [debug] [Thread-10 ]: Compiling test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__usage_mar_destination_history_destination_id__measured_month.fa03d71c0b
[0m16:47:51.950089 [debug] [Thread-10 ]: Writing injected SQL for node "test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__usage_mar_destination_history_destination_id__measured_month.fa03d71c0b"
[0m16:47:51.950770 [debug] [Thread-10 ]: finished collecting timing info
[0m16:47:51.951007 [debug] [Thread-10 ]: Began executing node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__usage_mar_destination_history_destination_id__measured_month.fa03d71c0b
[0m16:47:51.951226 [debug] [Thread-10 ]: finished collecting timing info
[0m16:47:51.952439 [debug] [Thread-10 ]: Finished running node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__usage_mar_destination_history_destination_id__measured_month.fa03d71c0b
[0m16:47:52.936092 [debug] [Thread-4  ]: On model.fivetran_log.fivetran_log__connector_daily_events: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "node_id": "model.fivetran_log.fivetran_log__connector_daily_events"} */


        select datetime_diff(
        cast(

        datetime_add(
            cast( timestamp_trunc(
        cast(
    current_timestamp
 as timestamp),
        day
    ) as datetime),
        interval 1 week
        )

 as datetime),
        cast(cast('2022-07-13' as date) as datetime),
        day
    )
[0m16:47:53.042686 [debug] [Thread-13 ]: Writing injected SQL for node "model.fivetran_log.fivetran_log__audit_table"
[0m16:47:53.043390 [debug] [Thread-13 ]: finished collecting timing info
[0m16:47:53.043641 [debug] [Thread-13 ]: Began executing node model.fivetran_log.fivetran_log__audit_table
[0m16:47:53.043862 [debug] [Thread-13 ]: finished collecting timing info
[0m16:47:53.044735 [debug] [Thread-13 ]: Finished running node model.fivetran_log.fivetran_log__audit_table
[0m16:47:53.045338 [debug] [Thread-11 ]: Began running node test.fivetran_log.not_null_fivetran_log__audit_table_unique_table_sync_key.f6ba3bbedc
[0m16:47:53.046023 [debug] [Thread-11 ]: Acquiring new bigquery connection "test.fivetran_log.not_null_fivetran_log__audit_table_unique_table_sync_key.f6ba3bbedc"
[0m16:47:53.046294 [debug] [Thread-28 ]: Began running node test.fivetran_log.unique_fivetran_log__audit_table_unique_table_sync_key.3a60f8fc5d
[0m16:47:53.046509 [debug] [Thread-11 ]: Began compiling node test.fivetran_log.not_null_fivetran_log__audit_table_unique_table_sync_key.f6ba3bbedc
[0m16:47:53.047239 [debug] [Thread-28 ]: Acquiring new bigquery connection "test.fivetran_log.unique_fivetran_log__audit_table_unique_table_sync_key.3a60f8fc5d"
[0m16:47:53.047489 [debug] [Thread-11 ]: Compiling test.fivetran_log.not_null_fivetran_log__audit_table_unique_table_sync_key.f6ba3bbedc
[0m16:47:53.047715 [debug] [Thread-28 ]: Began compiling node test.fivetran_log.unique_fivetran_log__audit_table_unique_table_sync_key.3a60f8fc5d
[0m16:47:53.054953 [debug] [Thread-11 ]: Writing injected SQL for node "test.fivetran_log.not_null_fivetran_log__audit_table_unique_table_sync_key.f6ba3bbedc"
[0m16:47:53.055211 [debug] [Thread-28 ]: Compiling test.fivetran_log.unique_fivetran_log__audit_table_unique_table_sync_key.3a60f8fc5d
[0m16:47:53.062806 [debug] [Thread-28 ]: Writing injected SQL for node "test.fivetran_log.unique_fivetran_log__audit_table_unique_table_sync_key.3a60f8fc5d"
[0m16:47:53.063351 [debug] [Thread-11 ]: finished collecting timing info
[0m16:47:53.063744 [debug] [Thread-11 ]: Began executing node test.fivetran_log.not_null_fivetran_log__audit_table_unique_table_sync_key.f6ba3bbedc
[0m16:47:53.064047 [debug] [Thread-11 ]: finished collecting timing info
[0m16:47:53.064929 [debug] [Thread-11 ]: Finished running node test.fivetran_log.not_null_fivetran_log__audit_table_unique_table_sync_key.f6ba3bbedc
[0m16:47:53.065519 [debug] [Thread-28 ]: finished collecting timing info
[0m16:47:53.065800 [debug] [Thread-28 ]: Began executing node test.fivetran_log.unique_fivetran_log__audit_table_unique_table_sync_key.3a60f8fc5d
[0m16:47:53.066026 [debug] [Thread-28 ]: finished collecting timing info
[0m16:47:53.066826 [debug] [Thread-28 ]: Finished running node test.fivetran_log.unique_fivetran_log__audit_table_unique_table_sync_key.3a60f8fc5d
[0m16:47:54.204389 [debug] [Thread-4  ]: Writing injected SQL for node "model.fivetran_log.fivetran_log__connector_daily_events"
[0m16:47:54.205058 [debug] [Thread-4  ]: finished collecting timing info
[0m16:47:54.205289 [debug] [Thread-4  ]: Began executing node model.fivetran_log.fivetran_log__connector_daily_events
[0m16:47:54.205509 [debug] [Thread-4  ]: finished collecting timing info
[0m16:47:54.206265 [debug] [Thread-4  ]: Finished running node model.fivetran_log.fivetran_log__connector_daily_events
[0m16:47:54.207017 [debug] [Thread-17 ]: Began running node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__connector_daily_events_connector_id__destination_id__date_day.9d2d1bd07f
[0m16:47:54.207681 [debug] [Thread-17 ]: Acquiring new bigquery connection "test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__connector_daily_events_connector_id__destination_id__date_day.9d2d1bd07f"
[0m16:47:54.207923 [debug] [Thread-17 ]: Began compiling node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__connector_daily_events_connector_id__destination_id__date_day.9d2d1bd07f
[0m16:47:54.208122 [debug] [Thread-17 ]: Compiling test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__connector_daily_events_connector_id__destination_id__date_day.9d2d1bd07f
[0m16:47:54.218816 [debug] [Thread-17 ]: Writing injected SQL for node "test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__connector_daily_events_connector_id__destination_id__date_day.9d2d1bd07f"
[0m16:47:54.219506 [debug] [Thread-17 ]: finished collecting timing info
[0m16:47:54.219752 [debug] [Thread-17 ]: Began executing node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__connector_daily_events_connector_id__destination_id__date_day.9d2d1bd07f
[0m16:47:54.219967 [debug] [Thread-17 ]: finished collecting timing info
[0m16:47:54.220721 [debug] [Thread-17 ]: Finished running node test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__connector_daily_events_connector_id__destination_id__date_day.9d2d1bd07f
[0m16:47:54.222977 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:47:54.223216 [debug] [MainThread]: Connection 'model.dbt_workshop_20221013.my_first_dbt_model' was properly closed.
[0m16:47:54.223442 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__account' was properly closed.
[0m16:47:54.223661 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__active_volume' was properly closed.
[0m16:47:54.223839 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__credits_used' was properly closed.
[0m16:47:54.224011 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__usage_cost' was properly closed.
[0m16:47:54.224180 [debug] [MainThread]: Connection 'model.fivetran_log.fivetran_log__connector_daily_events' was properly closed.
[0m16:47:54.224346 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__destination' was properly closed.
[0m16:47:54.224512 [debug] [MainThread]: Connection 'model.fivetran_log.fivetran_log__usage_mar_destination_history' was properly closed.
[0m16:47:54.224676 [debug] [MainThread]: Connection 'test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__usage_mar_destination_history_destination_id__measured_month.fa03d71c0b' was properly closed.
[0m16:47:54.224841 [debug] [MainThread]: Connection 'test.fivetran_log.not_null_fivetran_log__audit_table_unique_table_sync_key.f6ba3bbedc' was properly closed.
[0m16:47:54.225007 [debug] [MainThread]: Connection 'test.dbt_workshop_20221013.unique_my_first_dbt_model_id.16e066b321' was properly closed.
[0m16:47:54.225171 [debug] [MainThread]: Connection 'model.fivetran_log.stg_fivetran_log__connector' was properly closed.
[0m16:47:54.225335 [debug] [MainThread]: Connection 'test.fivetran_log.not_null_stg_fivetran_log__account_account_id.19a03e662a' was properly closed.
[0m16:47:54.225497 [debug] [MainThread]: Connection 'test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__connector_daily_events_connector_id__destination_id__date_day.9d2d1bd07f' was properly closed.
[0m16:47:54.225661 [debug] [MainThread]: Connection 'test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__active_volume_active_volume_id__destination_id.d6532db276' was properly closed.
[0m16:47:54.225823 [debug] [MainThread]: Connection 'test.fivetran_log.not_null_stg_fivetran_log__destination_destination_id.bf0be221ed' was properly closed.
[0m16:47:54.225995 [debug] [MainThread]: Connection 'test.fivetran_log.unique_stg_fivetran_log__destination_destination_id.f88b94a8cb' was properly closed.
[0m16:47:54.226160 [debug] [MainThread]: Connection 'test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__log_log_id__created_at.b6392465d9' was properly closed.
[0m16:47:54.226328 [debug] [MainThread]: Connection 'test.dbt_workshop_20221013.not_null_my_second_dbt_model_id.151b76d778' was properly closed.
[0m16:47:54.226492 [debug] [MainThread]: Connection 'test.dbt_workshop_20221013.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m16:47:54.226654 [debug] [MainThread]: Connection 'model.fivetran_log.fivetran_log__connector_status' was properly closed.
[0m16:47:54.226820 [debug] [MainThread]: Connection 'model.fivetran_log.fivetran_log__mar_table_history' was properly closed.
[0m16:47:54.226985 [debug] [MainThread]: Connection 'model.fivetran_log.fivetran_log__schema_changelog' was properly closed.
[0m16:47:54.227148 [debug] [MainThread]: Connection 'test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__mar_table_history_connector_id__destination_id__table_name__measured_month.2e5ecfcbac' was properly closed.
[0m16:47:54.227314 [debug] [MainThread]: Connection 'model.fivetran_log.fivetran_log__audit_table' was properly closed.
[0m16:47:54.227484 [debug] [MainThread]: Connection 'test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__connector_status_connector_id__destination_id.a2135f04e0' was properly closed.
[0m16:47:54.227649 [debug] [MainThread]: Connection 'test.fivetran_log.dbt_utils_unique_combination_of_columns_fivetran_log__schema_changelog_connector_id__destination_id__message_data__created_at.542291325d' was properly closed.
[0m16:47:54.227812 [debug] [MainThread]: Connection 'test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__credits_used_measured_month__destination_id.ad5114f878' was properly closed.
[0m16:47:54.227976 [debug] [MainThread]: Connection 'test.fivetran_log.dbt_utils_unique_combination_of_columns_stg_fivetran_log__usage_cost_measured_month__destination_id.3759bdc054' was properly closed.
[0m16:47:54.228152 [debug] [MainThread]: Connection 'test.fivetran_log.unique_fivetran_log__audit_table_unique_table_sync_key.3a60f8fc5d' was properly closed.
[0m16:47:54.249881 [info ] [MainThread]: Done.
[0m16:47:54.253403 [debug] [MainThread]: Acquiring new bigquery connection "generate_catalog"
[0m16:47:54.253726 [info ] [MainThread]: Building catalog
[0m16:47:54.257851 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:47:54.609637 [debug] [ThreadPool]: Acquiring new bigquery connection "fivetran-hands-on-lab.information_schema"
[0m16:47:54.610592 [debug] [ThreadPool]: Acquiring new bigquery connection "fivetran-hands-on-lab.information_schema"
[0m16:47:54.617746 [debug] [ThreadPool]: Acquiring new bigquery connection "fivetran-hands-on-lab.information_schema"
[0m16:47:54.624233 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:47:54.625074 [debug] [ThreadPool]: Acquiring new bigquery connection "fivetran-hands-on-lab.information_schema"
[0m16:47:54.628784 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:47:54.632336 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:47:54.636139 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:47:54.642086 [debug] [ThreadPool]: On fivetran-hands-on-lab.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "connection_name": "fivetran-hands-on-lab.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev`.__TABLES__
        where (upper(dataset_id) = upper('dbt_workshop_20221013_dev'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m16:47:54.643143 [debug] [ThreadPool]: On fivetran-hands-on-lab.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "connection_name": "fivetran-hands-on-lab.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.__TABLES__
        where (upper(dataset_id) = upper('dbt_workshop_20221013_dev_fivetran_log'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_fivetran_log`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m16:47:54.644191 [debug] [ThreadPool]: On fivetran-hands-on-lab.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "connection_name": "fivetran-hands-on-lab.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.__TABLES__
        where (upper(dataset_id) = upper('dbt_workshop_20221013_dev_stg_fivetran_log'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `fivetran-hands-on-lab`.`dbt_workshop_20221013_dev_stg_fivetran_log`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m16:47:54.645027 [debug] [ThreadPool]: On fivetran-hands-on-lab.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_workshop_20221013", "target_name": "dev", "connection_name": "fivetran-hands-on-lab.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `fivetran-hands-on-lab`.`fivetran_log`.__TABLES__
        where (upper(dataset_id) = upper('fivetran_log'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `fivetran-hands-on-lab`.`fivetran_log`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `fivetran-hands-on-lab`.`fivetran_log`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m16:47:59.119475 [info ] [MainThread]: Catalog written to /Users/angel.hernandez/Fivetran/dbt/dbt_workshop_20221013/target/catalog.json
[0m16:47:59.120119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12331dc70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123733520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123acb070>]}
[0m16:47:59.461088 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m16:47:59.461408 [debug] [MainThread]: Connection 'fivetran-hands-on-lab.information_schema' was properly closed.
[0m16:47:59.461621 [debug] [MainThread]: Connection 'fivetran-hands-on-lab.information_schema' was properly closed.
[0m16:47:59.461801 [debug] [MainThread]: Connection 'fivetran-hands-on-lab.information_schema' was properly closed.
[0m16:47:59.461972 [debug] [MainThread]: Connection 'fivetran-hands-on-lab.information_schema' was properly closed.


============================== 2022-10-13 16:48:03.813532 | 9eb83338-6192-4bd3-8faa-27f5118e379e ==============================
[0m16:48:03.813592 [info ] [MainThread]: Running with dbt=1.2.0
[0m16:48:03.814711 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/angel.hernandez/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'port': 8080, 'open_browser': True, 'which': 'serve', 'indirect_selection': 'eager'}
[0m16:48:03.815071 [debug] [MainThread]: Tracking: tracking
[0m16:48:03.841580 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11975df40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11975de50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11975db80>]}
[0m16:48:03.845252 [info ] [MainThread]: Serving docs at 0.0.0.0:8080
[0m16:48:03.845791 [info ] [MainThread]: To access from your browser, navigate to:  http://localhost:8080
[0m16:48:03.846170 [info ] [MainThread]: 
[0m16:48:03.846510 [info ] [MainThread]: 
[0m16:48:03.846873 [info ] [MainThread]: Press Ctrl+C to exit.
